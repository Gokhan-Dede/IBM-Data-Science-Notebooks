# -*- coding: utf-8 -*-
"""IBM Data Science Professional Certificate - Recap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KeXnobWIaQXioH5Ch_t6_aLyrk_d7Dqm

### DATA SCIENCE METHODOLOGY
---
"""

# Data Science is the study of large quantities of data, which reveals insights that help organizations make strategic decisions.

# The CRISP-DM Methodology - (Cross-industry Standard Process for Data Mining)
#   1-Business Understanding 2-Data Understanding 3-Data Preparation 4-Modeling 5-Evaluation - Deployment
#   We need to conduct two-sided evaluation between Business and Data Understandings, Data Preperation and Modeling, Evaluation and Business Understanding.

# Data Science Methodology

#   1-Business Understanding 2-Analytic Approach 3-Data Requirements 4-Data Collection 5-Data Understanding 6-Data Preparation 7-Modeling 8-Evaluation 9-Deployment 10-Feedback 11-Go back to Modeling if needed

#   We need to conduct two-sided evaluation between Data Requirements and Data Collection, Data Collection and Data Understanding, Data Preparation and Modeling, Modeling and Evaluation.


# Analytic Approach
#   Descriptive Approach                       - What happened?             (To gain Information)
#   Diagnostic Approach (Statistical Analysis) - Why did it happen?         (To gain Insight)
#   Predictive Approach (Forecasting)          - What will happen?          (To foresight)
#   Prescriptive Approach                      - How can we make it happen? (To optimize)


# Data Understanding
#   Sometimes a missing value might mean "no", or "0", or at other times simply means "we don't know". If a variable contains invalid or misleading values such as a numeric value called "age" that contains 0 to 100 and also 999,
#   where that "triple-9" actually means "missing", but would be treated as a valid value unless we corredted it.


# Data Preparation
#  - Data Cleansing : To work efficiently with the data, it must be prepared in a way that
#                                                                                         - addresses missing or invalid values
#                                                                                         - remove dublicates
#                                                                                         - ensure that everything is properly formatted
#  - During data preparation process;
#                                    - data scientist identify missing data
#                                    - data scientist define variables to be used in the model
#                                    - data scientists aggregate the data and merge them from different resources
#                                    - data scientists determine the timing of events



# Modeling
#   After defining which analytic approach will be used and data preparation, we need to define the most suitable machine learning model to answer the questions.


# Evaluation
#   A model evaluation can have two main phases: a diagnostic measure phase and statistical significance testing
#   A ROC (Receiver operating characteristic) curve is a useful diagnostic tool for determinig the optimal classification model.

"""### PYTHON

---
"""

#typecasting
a=float(2)
b=int(2.0)
c=str(1)
d=str(4.5)
print(type(a), type(b),type(c), type(d))

#Boolean
print(type(True), type(False))
a=int(True)
b=int(False)
c=bool(1)
d=bool(0)
print(a, b, c, d)

a=50-60
b=5*5
c=25/5
d=25//4
e=a**b
print(a, b, c, d, e)

#indexing with strings
a='Gokhan_Dd'
print(a[5], a[-5], a[0:2], a[1:8:2])

#length command
len("Gokhan_Dd")

#mathematical operation on strings
a='Esra'
b='Gokhan'
c=a+b+(2*a)+(3*b)
c

#escape command - \n
print('Michael Jackon \n is the best')
print('Michael Jackon \nis the best')

#tab command - \t
print('Michael Jackson \t is the best')
print('Michael Jackson \tis the best')

#placing \ in the string
print('Michael Jackson \\ is the best')
print('Michael Jackson \\is the best')
print(r'Michael Jackson \ is the best')

#methods for strings
a="Thriller is the sixth studio album"
b=a.upper()
c=a.replace('Thriller', 'Bad')
d=a.find('il')
e=a.find('the')
f=a.find('&de') #if substring is not in the string, the output is -1
print(a,b, c, d, e, f, sep=',')

#tuples
tuple1=(10, 9, 8, 5, 7, 2, 4, 0, 'disco', 1.2)
tuple2=('hard', 'rock', 20)

a=tuple1+tuple2
b=tuple1[1:4]+tuple2[0:1]
c=sorted(tuple1[0:8])

print(a, b, c, len(tuple1), sep=',')

#lists
L=['Michael Jackson', 10.1, 1982, [1,2], ('A', 1.2)]

a=L[0:5:2]
b=L.append(['pop',10])
c=L.extend(['pop',10])
del(L[0])

#we can convert a string to a list using split() command
d="Gokhan Dede".split()
e="Esra,Gokhan".split(",")

print(a, b, c, L, d, e, sep=',')

#aliasing - multiple names referring to the same object is known as aliasing.

A=[0,2,5,9]
B=[5,8,3,5,0]
A[0]=B[0]
A[0]

#clonning a list
A=["HARD ROCK", 5, 3, 65.5, 22]
B=A[:]
B

#dictionaries
DICT={"key1":"value", "key2":1, "key3":[0.5,2], "key4":(5,8), ("key5"):{"gokhan":"dede"}}
print(type(DICT), DICT, sep=",")

#keys must be immutable and unique; values can be immutable, mutable, and duplicates; eack key:value pair is seperated by a comma (,)

#we can add a new entry to the dictionary
DICT["Thriller"]=1982

#we can DELETE an entry from the dictionary
del(DICT["key1"])

#we can verify if an element is in the dictionary using the "in" command.
print(DICT)
print("key1" in DICT)

#we can use .keys() and .values() methods to get them
print(DICT.keys())
print(DICT.values())

#sets
Set1={"pop", "rock", "soul", "rock","r&b", "hard rock", "disco"}

#When the actual set is created, duplicate items will not be present.
print(Set1)

#you can convert a list to a set by using the function set(); this is also callsed as typecasting.
album_list=["Micheal Jackson", "Thriller", "Thriller", "1982"]
album_set=set(album_list)
print(album_set)

#set operations
A={"Thriller", "Back in Black", "AC/DC"}
A.add("NSYNC")
A.remove("AC/DC")
print(A, "AC/DC" in A, "Who" in A, sep=",")

#mathematical set operations
album_set1={"AC/DC", "Back in Black", "Thriller"}
album_set2={"AC/DC", "Back in Black", "The Dark Side of the Moon"}

album_set3=album_set1&album_set2  #we can use an ambersand (&) to find the intersection of two sets.
album_set4=album_set1.union(album_set2) #we can use .union() method to find the union of two sets.

print(album_set3, album_set4, album_set3.issubset(album_set4), sep=", ")  #we can check if a set is a subset using .issubset() method.

#comparison operations
a=6

print(a==7, a!=6, a<8, a<=6, a>5, a>=8, sep=", ")

#Branching

age=14

if (age>18):
  print("You can enter!")
elif (age==18):
  print("Go see Pink Floyd")
else:
  print("Move on!")

#Logic operators

# not operator; if input is True, the result is False; if input is False, the result is True.

# or operator; an or statement is only False if all boolean values are False.
album_year1=1990

if(album_year1<1980) or (album_year1>1989):
  print("The album 1 was made in the 70's or 90's")
else:
  print("The album 1 was made in the 1980's")

# and operator; an and statement is only True if all boolean values are True.
album_year2=1983
if (album_year2<1979) and (album_year2>1990):
  print("The album 2 was made in the 80's")

# for loops

squares=["red", "yellow", "green", "purple", "blue"]

for i in range(0,len(squares)):       # i (index number); range(first index number of the list, last index number of the list + 1); instead of range(0,5), we can write range(0,len(squares).
  squares[i]="white"                  # each value in the list is equalized to "white"

squares

# for loops with enumerate()
squares=["red", "yellow", "green", "purple", "blue"]

for i, square in enumerate(squares): # Enumerate returns an object that contains a counter as a key for each value within an object, because we often need the means to track loops, and the items accessed within that iteration of the loop
  print(i, square)

# while loops

squares=["orange", "orange", "orange", "purple", "blue"]
new_squares=[]

i=0
while (squares[i]=="orange"):         # a while loop will run if a condition is met.
  new_squares.append(squares[i])
  i=i+1

print(new_squares)

# python built-in functions

album_ratings=[10.0,8.5, 9.5, 7.0, 7.0, 9.5, 9.0, 9.5]

print(len(album_ratings), sum(album_ratings), sorted(album_ratings), sep=", ")

# if we use the method .sort(), the list album_ratings will change and no new list will be created.

# custom-built functions

def function_name(a):   #defining a basic function.
  b=a+1
  return b


def function_name2(a):   #we can create a documentation string to tell anyone who uses the function what the function does.
  """
  add 1 to a            
  """
  b=a+1
  return b

help(function_name2)     # we can use help() function to to display the documentation of the relevant function.


def functioon_name3(a, b):   # we can create a function with multiple parameters.
  c=a*b
  return c


def MJ():                     # In many cases, a function does not have a return statement.
  print("Michael Jackson")

def function_name4(x):        #  x - function variable, i - index, e - elements in the list
  for i, e in enumerate(x):
    print("Album", i, "Rating is", e)
album_rating=[10.0, 8.5, 9.5]
function_name4(album_rating)


def NoWork():      #Python doesn't allow a funtion to have an empty body, so we can use the keyword pass to run an empty function. The function returns a None.
  pass
print(NoWork())


def ArtistNames(*names):      # Variadic parameters allow us to input a variable number of elements. 
  for name in names:
    print(name)
ArtistNames("Michel Jackson", "AC/DC", "Pink Floyd")

from ast import Delete
# Scope

# The scope of a variable is the part of the program where that variable is accessible.
# The variable that are defined OUTSIDE of any function are said to be within the GLOBAL SCOPE, meaning they can be accessible anywhere after they are defined.
# LOCAL variables only exists within the scope of a function.

def addDC(x):
  x=x+"DC"
  print(x)
  return(x)

x="AC"        # Global variable
z=addDC(x)    # Within the scope of the function, the value of x is set to ACDC. The fuction returns the value and the value is assigned to z. After the value is returned, the scope of function is deleted.


def Thriller():
  Date=1982     # Local variable
  return Delete

Thriller()     # Local variable
#Date     # Prints error because it does not exist in the global scope.


def PinkFloyd():
  global claimed_sales            # If we define the variable with the keyword global, the variable will be a global variable.
  claimed_sales="45 millions"
  return claimed_sales

PinkFloyd()
print(claimed_sales)

# Exception handling

# try.......except statement - will firt attempt to execute the code in the "try" block, but if an error occurs, it will kick out and begin searching for the exception that maches the error.

try:
  getfile=open("myfile", "r")
  getfile.write("My file for exception handling.")
except IOError:
  print("Unable to open or read teh data in the file.")
except:
  print("Some other error occured!")


try:
  getfile=open("myfile", "r")
  getfile.write("My file for exception handling.")
except IOError:
  print("Unable to open or read teh data in the file.")
else:       # If we do not receive any messages that the program executed properly, we add else statement to give us notification.
  print("The file was written successfully!")
finally:    # We should close the file 
  getfile.close()                     
  print("The file is now closed!")

# Class

class Circle(object):                     # class Class_Name(class_parent)
  def __init__(self, radius, color):      # __init__ - special method or constructor used to initialize each instance of the class
    self.radius=radius                    # self - the self parameter refers to newly created instance of class
    self.color=color                      # radius, color - parameters to define attributes of an object

RedCircle=Circle(10, "red")
print(RedCircle.radius)                   # each parameter in the class used as a method to return the object attributes
print(RedCircle.color)


dir(RedCircle)                            # dir() function is useful for obtaining the list of data attributes and methods associated with a class

# File operations

# reading files with open
File_object= open("/file directory/file name.text", "mode")     # "mode" = "r" for reading, "w" for writing, "a" for appending

File1= open("/file directory/file name.text", "r")

File1.name    # we can use attributes to return their inputs
File1.mode
File1.close() # we should always close the file object using the method close()

with open("file_name.txt", "r") as File_1:    # Using a with statement to open a file is better practice because it automaticallt closes the file
  file_stuff=File_1.read()
  print(file_stuff)
print(File_1.closed)
print(file_stuff)

with open("file_name.txt", "r") as File_1:    
  file_stuff=File_1.readlines()               # We can output every line as an element in a list using the method readlines()
  print(file_stuff)



# writing files with open
File_object=open("/file directory/file name.text", "w")

File_1.write("This is line 1")           # We can apply method .write() to write the data to corresponding file.


with open("/file directory/file name.text", "w") as File_1:
  File_1.write("This is line A")


Lines=("This is line A\n", "This is line B\n", "This is line C\n")
with open("/file directory/file name.text", "w") as File_1:
  for line in Lines:
    File1.write(line)     # with a for loop, We can write each element in the list Lines within a new line.


with open("Example1.txt", "r") as read_file:        # we can copy one file to another as follows.
  with open("Example2.text", "w") as write_file:
    for line in read_file:
      write_file.write(line)

#Pandas

import pandas as pd

csv_path="file1.csv"            # It can be written with excel extension as well.
#df=pd.read_csv(csv_path)        # Create data frame by usind read.csv() method 

#df.head()                       # .head() method returns the first five rows of data frame.


songs={"Album":["Thriller", "Back in Black", "The bodyguard"], "Released":[1982, 1980, 1992], "Length":["00:42:19", "00:42:11", "00:57:44"]}
df=pd.DataFrame(songs) # We can create a data frame out of a dictionary by using .DataFrame() method.

x_dataframe=df[["Album", "Released"]]      # We can create a new data frame consisting of specified columns of predefined data frame.

a0=x_dataframe.loc[[0,1]]              # We can use loc to select specific rows of the DataFrame  based on their INDEX LABELS.
a1=x_dataframe.loc[[0], ["Album"]]     # We can use loc to select specific rows and specific columns of the data frame based on their INDEX LABELS.
a2=x_dataframe.loc[ :1, :"Released"]   # We can use loc with the to select ranges of rows and columns of the data frame based on their INDEX LABELS.

print(a0, "\n"*2,  a1, "\n"*2, a2, "\n"*2)

b0=x_dataframe.iloc[0,0]               # We can access the specific row and column of a dataframe with .iloc[row index, column index] method based on their INTEGER INDEX NUMBERS.
b1=x_dataframe.iloc[0:3, 0:1]          # We can access the specific range of rows and columns of a dataframe with .iloc[row index1:row index2, column index1:column index2] method based on their INTEGER INDEX NUMBERS.

print(b0, "\n"*2)
print(b1)

x_dataframe.to_csv    # We can save a data frame as a csv file by using .to_csv() method

# Numpy - One Dimensional 

import numpy as np

a=np.array([0,1,2,3,4])     # we can convert a list into an array by using np.array(list or list name) function

type(a)                     # we can see the type of variable via type() function
print(a.dtype)              # we can see the data types within the numpy array
print(a.size)               # we can see the number of elements within the numpy array
print(a.ndim)               # we can see the dimension of the numpy array
print(a.shape)              # we can see the shape of the numpy array including row and column numbers


a[1]=5                      # We can select an element by using its index number and redefine selected element in the numpy array 
a[2:4]=10,12                # We can divide a numpy array to redefine the selected elements 
print(a)

b=a[3:5]                    # We can define a new numpy array with the selected elements of another array
print(b) 


# Vector Addition
u=[1,0]                     # Addition by using lists
v=[0,1]
z1=[]
for n, m in zip(u,v):     
  z1.append(n+m)

u=np.array([1,0])           # Addition by using arrays
v=np.array([0,1])
z1=u+v

print("z1: ", z1)


# Vector Substraction
u=[1,0]                     # Substraction by using lists
v=[0,1]
z2=[]
for n, m in zip(u,v):     
  z2.append(n-m)

u=np.array([1,0])           # Substraction by using arrays
v=np.array([0,1])
z2=u-v

print("z2: ", z2)


# Array Multiplication with a scalar
y=[1,2]                    # Multiplication by using a list
z3=[]
for n in y:
  z3.append(2*n)

y=np.array([1,2])          # Multiplication by using an array
z3=2*y

print("z3: ", z3)



# Hadamard Product of two numpy arrays - Multiplication of two arrays
u=[1,2]                    # Multiplication of two lists
v=[3,2]
z4=[]
for n, m in zip(u,v):
  z4.append(n*m)

u=np.array([1,2])          # Multiplication of two arrays
v=np.array([3,2])
z4=u*v
print("z4: ", z4)



# Dot product of an array = It is a single number given by the following teram and represents how similar to verctors are.
#x=np.array([a,b]) , y=np.array([c,d]) - dot product of x and y - (x*T.y)=(axc)+(bxd)
u=np.array([1,2])                         
v=np.array([3,5])
result=np.dot(u,v)
print("dot product: ", result)



# Adding constant to a numpy array - Broadcasting

u=np.array([1,2,3,-1])
z5=u+3                  # If we add a scalar value to the array, numpy will add it to each element in the array. This property is known as broadcasting.
print(z5)



# Universal functions
a=np.array([1,-2,3,4,5])
min_a=a.min()
mean_a=a.mean()
max_a=a.max()
print(min_a, mean_a, max_a, sep=", ")

pi=np.pi
x=np.array([0,pi/2,pi])
y=np.sin(x)
print(y)

v=np.linspace(0,2*np.pi,100)      # linspace(first number, last number, number of samples between the range) function returns evenly spaced numbers over specified interval.
z=np.sin(v)                       # We can use linspace() function to generate 10 evenly spaced samples from the interval zero to two pi.
import matplotlib.pyplot as plt   # We use the command .plot() from the matplotlib.pyplot library to display the plot
plt.plot(x,y)

# Numpy - Two Dimensional 

a=[[11,12,13], [21,22,23], [31,32,33]]

A=np.array(a)

print(A, "\n")
print(A.ndim, A.shape, A.size, "\n", sep=", ")

print(A[1][2], "\n")         #we can access the different elements of array with array_name[row index][column index]

print(A[0:2, 1:3], "\n")     #we can also use slicing in a numpy array


x=np.array([[-1,0],[1,-2]])
y=np.array([[2,3], [5,4]])
z=x+y
v=2*y
w=x*y #Hadamard product
print(z, "\n"*2, v, "\n"*2, w, "\n"*2)

B=np.array([[0,1,1],[1,0,1]])       #we must make sure number of columns in B equals to the number of rows in array C TO HAVE DOT PTODUCT OF B AND C
C=np.array([[1,1],[1,1],[-1,1]])
D=np.dot(B,C)
print(D)

# Rest API (Representational State Tranfer Application Programming Interface )

# HTTP is a set of protocols designed to enable communication between clients and servers. It works as a request-response protocol between a client and server.
# So, to request a response from the server, there are mainly two methods:
# GET : to request data from the server.
# POST : to submit data to be processed to the server.

# to make HTTP requests in python, we can use several HTTP libraries like:
#       -httplib
#       -urllib
#       -requests


# Making a GET request
import requests                    # we need to import requests library
url='https://www.ibm.com'          # https:// (Hypertext Transfer Protocol Secure) - scheme for the protocol  |  internet adress or base url (Uniform Resource Protocol) is used to find the location.
response=requests.get(url)         # requests.get() retrieves data from the server, retrieved data is generalled called as payload

response.url                       # we can see the response url via .url command              
response.status_code               # .status_code returns the status of the request. 1XX:Informational Class, 100:Everything so far is OK | 2XX:Success Class, 200:OK | 3XX:Redirection Class, 300: Multiple Choices | 
                                   #                                                 4XX:Client Error Class, 401:Unauthorized, 403:Forbidden, 404:Not Found, 405:Not Allowed | 500:Server Error | 501:Not Implemented
response.headers                   # .headers returns a dictionary data contains information about requests and response
response.content                   # .content raw format of retrieved payload
response.encoding                  # .encoding provides the encoding type of payload so that we can know if we need to convert our data into string encoding formats like UTF-8, UTF-16 OR UTF-32.
response.text                      # response data is generally in json (JavaScript Object Notation) format, so we can retrieved data as text format by using .text command.


headers=response.headers           # we can assign headers to a variavle to define it as object 
headers["Date"]                    # we can request values of corresponsing keys in the headers dictionary
headers["Content-Type"]  



# Customizing result of GET request
url_get='http://httpbin.org/get'        # we have the base url with get, get works as the ROUTE for the location on the web server
payload={"name":"Joseph", "ID":"123"}   # we use the dictionary to create a query string
response2=requests.get(url_get, params=payload)

response2.url
response2.status_code
response2.headers
response2.encoding
response2.content                       # we can see added key and value pairs in the content of response



# Making a POST request
import requests

# defining the api-endpoint 
API_ENDPOINT = "http://pastebin.com/api/api_post.php"
  
# your API key here
API_KEY = "XXXXXXXXXXXXXXXXX"
  
# your source code here
source_code = '''
print("Hello, world!")
a = 1
b = 2
print(a + b)
'''
# data to be sent to api
data = {'api_dev_key':API_KEY,
        'api_option':'paste',
        'api_paste_code':source_code,
        'api_paste_format':'python'}
  
# sending post request and saving response as response object
r = requests.post(url = API_ENDPOINT, data = data)
  
r.url
r.status_code     # The POST command is not allowed because of authorization, but you can post data via defined procedure.




#Other HTTP Methods
#Aside from GET and POST, other popular HTTP methods include PUT, DELETE, HEAD, PATCH, and OPTIONS.

requests.put('https://httpbin.org/put', data={'key':'value'})         # Updates data already on the server, or writes new data on the server by also its corresponding url
requests.delete('https://httpbin.org/delete')                         # Delete data on the server
requests.head('https://httpbin.org/get')                              # Head method asks for a response with header information but without the response body
requests.patch('https://httpbin.org/patch', data={'key':'value'})     # Patch method is used to modify a specific data in the server to produce a new version    
requests.options('https://httpbin.org/get')                           # Options method requests permitted communication options for a given URL or server

# HTML (Hypertext Markup Language) for Webscraping

# HTML describes the elements of a page through tags characterized by angle brackets.
#                                <html>
#                                <head>
#                                <title>
#                       Parent   <body> 
#                                <p>Paragraph<p>
#                                <a>tag specialized for HTML links<a> - we can add an html link to the paragraph tagged with <p>, and it has the href attribute to specify the link.
#                                <img src="https://xxxxxxxxx.jpg">    - we can insert an image to the paragraph tagged with <p>, and it has the attribute src to specify the URL of the image.
#                                <style>                              - The <style> tag is used to define style information (CSS) for a document.
#                                  h1 {color:red;}
#                                  p {color:blue;}
#                                <h1>First header</h1>
#                                <table>                              - The <table> tag defines an HTML table.
#                                  <tr>                               - The <tr> element defines a table row
#                                    <th>Month</th>                   - the <th> element defines a table header
#                                    <th>Savings</th>
#                                  </tr>
#                                  <tr>
#                                    <td>January</td>
#                                    <td>$100</td>                     - <td> element defines a table cell
#                                  </tr>
#                                </table>                              - An HTML table may also include <caption>, <colgroup>, <thead>, <tfoot>, and <tbody> elements.
#                                <h2>Second header</h2>
#                                <h3>Third header</h2>
#                                </body>
#                                </head>
#                                </html>



# HTML lists allow web developers to group a set of related items in lists.

# An unordered HTML list
# An unordered list starts with the <ul> tag. Each list item starts with the <li> tag.
# <ul>
#   <li>Coffee</li>
#   <li>Tea</li>
#   <li>Milk</li>
# </ul>


# An ordered HTML list
#An ordered list starts with the <ol> tag. Each list item starts with the <li> tag.
#<ol>
#  <li>Coffee</li>
#  <li>Tea</li>
#  <li>Milk</li>
#</ol>

from pandas.core.api import BooleanDtype
# Webscraping with BeautifulSoup - Beautiful Soup is a Python library for parsing (ayrıştırmak) STRUCTURED DATA. 

#   1-Find the URL that you want to scrape
#   2-Inspect the page - right click on the page and click on the "Inspect"
#   3-Find the data you want to extract
#   4-Write the python code


# There are different types of html parsers.
# 1. Python’s html.parser -	BeautifulSoup(markup, "html.parser")	                            - Moderately fast - Not as fast as lxml, less lenient than html5lib.
# 2. lxml’s HTML parser	  - BeautifulSoup(markup, "lxml")	                                    - Very fast - Lenient
# 3. lxml’s XML parser	  - BeautifulSoup(markup, "lxml-xml") | BeautifulSoup(markup, "xml")  - Very fast - The only currently supported XML parser (XML - Extensible Markup Language)
# 4. html5lib	            - BeautifulSoup(markup, "html5lib")	                                - Extremely lenient - Parses pages the same way a web browser does - Creates valid HTML5 - Very slow 


#a_tags=soup.find_all(name="a") # find_all(name, attrs, recursive, string, limit, **kwargs) | name:      tag names, 
                                                                                           # attrs:     the attributes that tag has such as id, href.... 
                                                                                           # recursive: it is instructing beautifulsoup to check the children of a particular node for matches (you can set it True or False)
                                                                                           # string:    we can search for specific string caharacters to parse them
                                                                                           # limit:     argument is used when we don’t need all the results, and with it, we can stop searching for results after a certain number
                                                                                           # **kwargs:  when programming, you may not be aware of all the possible use cases of your code, and may want to offer more options for future programmers working with the module. 
                                                                                           #            we can pass a variable number of arguments to a function by using *args and **kwargs in our code. 
                                                                                           #            the single-asterisk form of *args can be used as a parameter to send a non-keyworded variable-length argument list to functions.  the asterisk (*) is the important element here, as the word args is can be changed.
                                                                                           #            the double asterisk form of **kwargs is used to pass a keyworded, variable-length argument dictionary to a function. The two asterisks (**) are the important element here, s the word args is can be changed.


import requests
from bs4 import BeautifulSoup

url="http://www.ibm.com"
response=requests.get(url)

soup=BeautifulSoup(response.content, "html.parser")  # create a beautifulsoup object with the extracted content of webscrapping to use it for parsing

#print(soup.prettify())                               # we can use the .prettify() method to display the html in nested structure

# title of the page
title_tag=soup.title
print("title: ", title_tag)

# get attributes:
title_tag_attributes=soup.title.name
print("attributes of title: ", title_tag_attributes)

# get content of a tag as string values:
title_tag_content=soup.title.string
print("content of title: ", title_tag_content)

#get parent tags of a tag
title_parents=soup.title.parent
print("parent tags in title: ", title_parents)

#get attributes of a parent tag
title_parent_attribute=soup.title.parent.name
print("children tags in title: ", title_parent_attribute)


# Another common task is to grab links. For example:
for url in soup.find_all('a'):
    print(url.get('href'))           # we can use .get('href') to get the true URL.        


# Finally, you may just want to grab text. You can use .get_text() on a Beautiful Soup object, including the full soup:
print(soup.get_text())


# A table example, as well as scraping XML documents.
# table = soup.find('table')
# print(table)
# table_rows = table.find_all('tr') #we can find the table rows within the table
# for tr in table_rows:             # we can iterate through the rows, find the td tags, and then print out each of the table data tags
#     td = tr.find_all('td')
#     row = [i.text for i in td]
#     print(row)

"""### SQL (Structured Query Language)

---


"""

# SQL Statements are used for interacting with entities(tables), attributes(columns), and their tuples(rows with data values) in relational databases.

# TYPE OF SQL STATEMENTS

#   SQL commands are mainly categorized into five categories as: 

#       1- DDL – Data Definition Language
#        DDL or Data Definition Language actually consists of the SQL commands that can be used to define the database schema. 
#        It simply deals with descriptions of the database schema and is used to create and modify the structure of database objects in the database. 
#        DDL is a set of SQL commands used to create, modify, and delete database structures but not data. 
#        These commands are normally not used by a general user, who should be accessing the database via an application.

#        List of DDL commands: 
#                             CREATE:   This command is used to create the database or its objects (like table, index, function, views, store procedure, and triggers).
#                             DROP:     This command is used to delete objects from the database.
#                             ALTER:    This is used to alter the structure of the database.
#                             TRUNCATE: This is used to remove all records from a table, including all spaces allocated for the records are removed.
#                             COMMENT:  This is used to add comments to the data dictionary.
#                             RENAME:   This is used to rename an object existing in the database.


#       2- DQL – Data Query Language
#        DQL statements are used for performing queries on the data within schema objects. 
#        The purpose of the DQL Command is to get some schema relation based on the query passed to it. 
#        We can define DQL as follows it is a component of SQL statement that allows getting data from the database and imposing order upon it. 
#        It includes the SELECT statement. This command allows getting the data out of the database to perform operations with it. 

#        List of DQL: 
#                    SELECT: It is used to retrieve data from the database.



#       3- DML – Data Manipulation Language
#        The SQL commands that deals with the manipulation of data present in the database belong to DML or Data Manipulation Language and this includes most of the SQL statements. 
#        It is the component of the SQL statement that controls access to data and to the database. Basically, DCL statements are grouped with DML statements.

#        List of DML commands: 
#                             INSERT :       It is used to insert data into a table.
#                             UPDATE:       It is used to update existing data within a table.
#                             DELETE :      It is used to delete records from a database table.
#                             LOCK:         Table control concurrency.
#                             CALL:         Call a PL/SQL or JAVA subprogram.
#                             EXPLAIN PLAN: It describes the access path to data.



#       4- DCL – Data Control Language
#        DCL includes commands such as GRANT and REVOKE which mainly deal with the rights, permissions, and other controls of the database system. 

#        List of  DCL commands: 
#                              GRANT: This command gives users access privileges to the database.
#                              REVOKE: This command withdraws the user’s access privileges given by using the GRANT command.



#       5- TCL – Transaction Control Language
#        Transactions group a set of tasks into a single execution unit. Each transaction begins with a specific task and ends when all the tasks in the group successfully complete. 
#        If any of the tasks fail, the transaction fails. Therefore, a transaction has only two results: success or failure. 

#        List of TCL commands: 
#                             COMMIT: Commits a Transaction.
#                             ROLLBACK: Rollbacks a transaction in case of any error occurs.
#                             SAVEPOINT: Sets a save point within a transaction.
#                             SET TRANSACTION: Specifies characteristics for the transaction.

# 1- DDL – Data Definition Language

#        List of DDL commands: 
#                             CREATE:   This command is used to create the database or its objects (like table, index, function, views, store procedure, and triggers).
#                             DROP:     This command is used to delete objects from the database.
#                             ALTER:    This is used to alter the structure of the database.
#                             TRUNCATE: This is used to remove all records from a table, including all spaces allocated for the records are removed.
#                             COMMENT:  This is used to add comments to the data dictionary.
#                             RENAME:   This is used to rename an object existing in the database.


#CREATE Statement

# CREATE TABLE table_name

# CREATE TABLE table_name(id char(2)) PRIMARY KEY NOT NULL, name varchar(24))    Note: id and name are column names; 
#                                                                                      char(2) defines a character string with a fixed length; 
#                                                                                      varchar(24) defines character string with variable length
#                                                                                      PRIMARY KEY prevents to have repeated values in each row a column

# CREATE TABLE author(
#     author_id CHAR(2) PRIMARY KEY NOT NULL,
#     lastname VARCHAR(15) NOT NULL,
#     firstname VARCHAR(15) NOT NULL,
#     email VARCHAR(40),
#     city VARCHAR(15),
#     country CHAR(2)  
#     )




# ALTER Statement
# We can use it to add or remove columns, modify the data type of coluns, add or remove keys, and add or remove constraints.

# Syntax to add column
# ALTER TABLE table_name
#   ADD COLUMN column_name datatype
#   .........
#   ADD COLUMN column_name datatype;


# Syntax to modify the data type of a column
# ALTER TABLE table_name
#   ALTER COLUMN column_name SET DATA TYPE CHAR(20);


# Syntax to modify column name in a table
# ALTER TABLE column_name
#   RENAME COLUMN current_column_name TO new_column_name;


# Syntax to remove a column
# ALTER TABLE table_name
#   DROP COLUMN column_name;




# DROP Statement
# we can delete a table by using DROP statement.

# DROP TABLE table_name;




# TRUNCATE Statement
# We can clean a table by using TRUNCATE statement. It delete all row values of a table.

# TRUNCATE TABLE table_name
#    IMMADIATE;

# 2- DQL – Data Query Language

#        List of DQL: 
#                    SELECT: It is used to retrieve data from the database.

# SELECT * from table_name 

# SELECT column_name1, column_name2, column_name3 from table_name;

# SELECT column_name1, column_name2 from table_name WHERE condition_definition

# 3- DML – Data Manipulation Language

#        List of DML commands: 
#                             INSERT :       It is used to insert data into a table.
#                             UPDATE:       It is used to update existing data within a table.
#                             DELETE :      It is used to delete records from a database table.
#                             LOCK:         Table control concurrency.
#                             CALL:         Call a PL/SQL or JAVA subprogram.
#                             EXPLAIN PLAN: It describes the access path to data.


# INSERT Statement

# We can insert one value for specified each column.
# INSER INTO table_name
#   (column_name1, column_name2, column_name3)
# VALUES (value1, value2, value3)

# We can insert multiple values for specified each column.
# INSER INTO table_name
#   (column_name1, column_name2, column_name3)
# VALUES 
#   ("value1"...........)                     Note: Values to add first column.
#   ("value2"...........)                           Values to add second column.                            
#   ("value3"...........)                           Values to add third column.




# UPDATE Statement

# UPDATE table_name
# SET column_name1="value1"
#     column_name2="value2"
#     WHERE column_name3="value3"           Note: If we do not use a where clause, it updates all rows with value1 and value2 in the specified columns.  




# DELETE Statement

# We can remove one or more ROWS from a table.
# DELETE from table_name
#     WHERE column_name IN ("value1", "value2")

# LIMIT statement

# It is used for restricting the number of rows from the database.

# SELECT * from table_name LIMIT 10;

# SELECT * from table_name WHERE column_name="value" LIMIT 5;

# SELECT * from table_name LIMIT 15 OFFSET 10;   -   This statement retrieves 15 rows starting from 11th row.

# Aggregate Functions with SELECT Statement

# The SQL aggregate functions —  COUNT, DISTINCT, SUM, MAX, MIN, AVG — all return a value computed or derived from one column’s values, after discarding any NULL values. 

# The syntax of all these functions except DISTINCT is:
# SELECT AGGREGATE_FUNCTION('column_name') FROM TABLE_NAME


# COUNT(column_name)function
# It is a built-in function that retrieves the NUMBER of row maching the query criteria. 

# SELECT COUNT(column_name) from table_name WHERE column_name="value";



# DISTINCT function
# It is used to remove deplicate values from a result set.

# SELECT DISTINCT columnname from table_name WHERE column_name="value";



# SUM(column_name) function
# It adds up all the values in a column

# SELECT SUM(column_name) from table_name;

# SELECT SUM(column_name) as desired_column_name from table_name;



# MAX(column_name) and MIN(column_name) functions

# SELECT MAX/MIN(column_name) from table_name WHERE column_name=value



# AVG(column_name) function

# SELECT AVG(column_name) from table_name;

# SELECT AVG(column_name1/column_name2) from table_name WHERE column_name="value";        Note: We can conduct mathematical operations between columns.

# Scalar and String Functions

# Scalar and String Functions are ROUND, LENGTH, UCASE AND LCASE. These functions perform operations on every input value.
 
# The syntax of all these is THE SAME AS FOLLOWS:
# SELECT SCALAR/STRING_FUNCTION('column_name') FROM TABLE_NAME;                  Note: We can use them with other functions and WHERE clause too.


# SELECT ROUND(column_name) from table_name;
# SELECT LENGTH(column_name) from table_name;
# SELECT UCASE(column_name) from table_name;
# SELECT LCASE(column_name) from table_name;

# SELECT * from table_name WHERE LCASE(column_name)="value";

#SELECT DISTINCT(UCASE(column_name)) from table_name;

# Date and Time Built-in Functions

# Most databases contain special datatypes for dates and times. For example, Db2 contains date, time and timestamp.

# Built-in date/time functions are YEAR(), MONTH(), DAYOFMONTH(), DAYOFWEEK(), DAYOFYEAR(), WEEK(), HOUR(), MINUTE(), SECOND()

# General syntax for date/time functions as follows,
# SELECT DAY(column_name) from table_name WHERE column_name="value";


# Date/time functions can be used in WHERE clause.
# SELECT COUNT(*) from table_name WHERE MONTH(column_name)="value";


# Date/Time functions can be used for arithmetically.
# SELECT (column_name +3 DAYS) from table_name;


# We can fing how many days have passed since a date/time till now.
# SELECT (CURRENT_DATE/CURRENT_TIME-column_name) from table_name;

# Sorting Results

# SELECT column_name from table_name ORDER BY column_name;                       Note: This syntax returns results in an ascending order. (Artan)
# SELECT column_name from table_name ORDER BY column_name DESC;                  Note: This syntax returns results in an descending order. (Azalan)

# SELECT column_name1, column_name2 from table_name ORDER BY 2;                  Note: This syntax returns results in and ascending order according to column 2.

# Grouping Results

# SELECT column_name, COUNT(column_name) from table_name GROUP BY column_name;  Note: This syntax counts the same values in the selected column and return each different value with their corresponding repetition times.

# SELECT column_name, COUNT(column_name) from table_name 
# AS column_name from table_name GROUP BY column_name;                          Note: This syntax does the same thing with previous syntax. In addition to that, we give a name for the result column by using AS statement.


# SELECT column_name, COUNT(column_name)
# AS column_name from table_name GROUP BY column_name
# HAVING COUNT (column_name) >= "value";                                        Note: We can write a condition for GROUP BY statement by using HAVING COUNT statement so that we can create group in the previously groupped results.

# Sub-Queries and Nested Select Statements

# Sub-query is a query inside another query.

# SELECT column_name1 from table_name WHERE column_name2=(SELECT MAX(column_name2) from table_name);



# Sub-query to evaluate aggregate functions
# We can not evaluate aggregate functions like AVG() in the WHERE clause; therefore, we use sub-elect expression

# SELECT column_name1, column_name2, column_name3 from table_name WHERE column3 < (SELECT AVG(column_name3) from table_name);



# Sub-queries in the list of columns
# We can write a SELECT statement inseted of a column name to specify a specific result with a desired column name by using also AS statement in the sub-query.

# SELECT column_name1, column_name2, (SELECT AVG(column_name2) from table_name) AS column_name3 from table_name;



# Sub-queries after FROM clause

# SELECT * from (SELECT column_name1, column_name2 from table_name) AS column_name3;

# Working with Multiple Tables

# There are three ways to access multiple tables in the same query
#   1- Sub-queries
#   2- Implicit JOIN
#   3- JOIN operators (INNER JOIN, OUTER JOIN, etc.)



# 1- Accessing Multiple Tables with Sub-queries

# SELECT * from table_name1 WHERE column_name1 IN (SELECT column_name2 from table_name2);

# SELECT * from table_name1 WHERE column_name1 IN (SELECT column_name2 from table_name2 WHERE column_name="value";



# 2- Accessing Multiple Tables with Implicit JOIN

# SELECT * from table_name1, table_name2;                                       Note: The result is a full join or cartesian/cross join - Every row in the first table is joined with every row in the second table.
#                                                                                      Each row will be matched seperately with the each row of the second table, so there will be more result rows than in both tables.



# We can use additional operand to limit the result set

# SELECT * from table_name1, table_name2 WHERE table_name1.column_name1=table_name2.column_name2;



#We can use shorter aliases for table names with a different syntaxes 

# SELECT * from table_name1 T1, table_name2 T2 WHERE T1.column_name1=T2.column_name2;

# SELECT column_name1, column_name2 from table_name1 T1, table_name2 T2 WHERE T1.column_name1=T2.column_name2;

# SELECT T1.column_name1, T2.column_name2 from table_name1 T1, table_name2 T2 WHERE T1.column_name1=T2.column_name2;

"""### Accessing Databases using DB-API

---


"""

# DB-API is Python's standard API for accessing relational databases, and a standard that allows to write a single program that works with multiple kinds of relational databases instead of writing seperate programs for each one.

# There is a mechanism by which the Python program communicates with the DBMS (Database Management System):
#     - The application program begins its database access with one or more API calls that connect the program to the DBMS.
#     - Then to send the SQL statement to the DBMS, the program builds the statement as a text string and then makes an API call to pass the contents to the DBMS.
#     - The application program makes API calls to check the status of its DBMS request and to handle errors.
#     - The application program ends its database access with an API call that disconnects it from the database.



# The two main concepts in the Python DB-API are:

# 1- Connection objects used for
#     - Connect to a database
#     - Manage your transactions.

# Following are a few connection methods:
#     - cursor(): This method returns a new cursor object using the connection.
#     - commit(): This method is used to commit any pending transaction to the database.
#     - rollback(): This method causes the database to roll back to the start of any pending transaction.
#     - close(): This method is used to close a database connection.



# Syntax for connection objects

# from dbmodule import connect

# connection = connect('databasename', 'username', 'pswd')  Note: Create connection object
  
# cursor = connection.cursor()                              Note: Create a cursor object
  
# cursor.execute('select * from mytable')                   Note: Run queries
# results = cursor.fetchall()
  
# cursor.close()                                            Note: Free resources
# connection.close()




# 2- Cursor object

# It is an object that is used to make the connection for executing SQL queries. 
# It acts as middleware between SQLite database connection and SQL query. 
# It is created after giving connection to SQLite database.

# The API used for cursor objects.

# close
# cursor.close()

# execute
#   Prepares and executes a query.
#   cursor.execute()

# executemany
#   Prepares a database operation and executes it against all parameter sequences found in the sequence seq_of_parameters.
#   cursor.executemany()

# fetch - çekmek, almak
#   Fetches the next row of a query result set, returning a single sequence, or None when no more data is available.
#   cursor.fetch()

# fetchmany
#   Fetches the next set of rows of a query result, returning a sequence of sequences, for example, a list of tuples. An empty sequence is returned when no rows are available.
#   cursor.fetchmany()

# fetchall
#   Fetches all (remaining) rows of a query result, returning them as a sequence of sequences.
#   cursor.fetchall()

# description
#   A read-only attribute describing the column information.
#   cursor.description()

# nextset
#   Forces the cursor to skip to the next available set, discarding any remaining rows from the current set.
#   cursor.nextset()

# arraysize
#   This read/write attribute specifies the number of rows to fetch at a time with fetchmany(). It defaults to 1 which indicates to fetch a single row at a time.
#   cursor.arraysize()

# callproc
#   Calls a stored database procedure with the given name. After fetching all the result sets and rows, use the proc_status attribute to check the status result of the stored procedure.
#   cursor.callproc()

# proc_status
#   Read/write attribute specifies the number of rows to fetch at a time with fetchmany(). Defaults to 1 which indicates to fetch a single row at a time.
#   cursor.proc_status





# Writing code using DB-API
#
# from dbmodule import connect                                  Note: We import database module by using the connect API from dbmodule
# 
# connection=connect("database_name", "username", "password")   Note: To open a connection to the database, we use connect constructor and pass information in the parameters.
#
# cursor=connection.cursor()                                    Note: We define a cursor object on the connection object.
#
# cursor.execute(SELECT * from table_name)                      Note: We use cursor object to run queries with .execute() method.
#
# results=cursor.fetchall()                                     Note: We use the cursor object to fetch all results.
#
# cursor.close()                                                Note: We close the cursor object.
# connection.close()                                            Note: We close the connection to free resources.

"""### Data Analysis with Python

---


"""

# Python Packages for Data Science

# 1- Scientific Computing Libraries
#    - Pandas (Data structures and tools)
#    - Numpy (Arrays and matrices)
#    - SciPy (Integrals, solving differential equations, optimization)

# 2- Visualization Libraries
#    - Matplotlib (plots and graphs)
#    - Seaborn (plots: heat maps, time series, violin plots)

# 3- Algorithmic Libraries
#    - Scikit-learn (Machine Learning: regression, classification)
#    - Statsmodels (Explore data, estimate statistical models and perform statistical tests)

# Importing and Exporting Data in Python

# Data Format   |      Read        |       Save
# csv               pd.read_csv()       df.to_csv()
# json              pd.read_json()      df.to_json()
# excel             pd.read_excel()     df.to_excel()
# sql               pd.read_sql()       df.to_sql()

# Data Acquisition: A process of loading and reading data into notebook from various sources.
# To read any data using Python's package, there are two important factors to consider: format and file path

# Importing a CSV into Python
import pandas as pd
url="https://....................."
df=pd.read_csv(url, header=None)              # Note: header should be set to None if you do not want to set first row as a header automatically.

# Printing the dataframe in Python
# df prints the entire dataframe but it is not recomended for large datasets.

df.head()   # it returns the first 5 rows of a dataset
df.tail()   # it returns the last 5 rows of a dataset

# Adding headers

# we can assign column names in pandas or we can create a list to assign as column names.

headers=[".....,.....,......,.....,...."]
df.columns=headers

# Exporting a Pandas dataframe to CSV

path= "C:/Windows/......../automobile.csv"    # Note: Specify the file path which includes the file name that you want to write to.
df.to_csv(path)

# Getting Started Analyzing Data in Python

# We should check the data types to consider potential info and type mismatch.
# We should check the data types to know which data is compatible with which python methods

# Pandas dtype	   |  Python type	  |   NumPy type	                                                      |   Usage
# object	            str or mixed	    string_, unicode_, mixed types	                                      Text or mixed numeric and non-numeric values
# int64	              int	              int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64	      Integer numbers
# float64	            float	            float_, float16, float32, float64	                                    Floating point numbers
# bool	              bool	            bool_	                                                                True/False values
# datetime64	        NA	              datetime64[ns]	                                                      Date and time values
# timedelta[ns]	      NA	              NA	                                                                  Differences between two datetimes
# category	          NA	              NA	                                                                  Finite list of text values


df.dtypes     # Note: If we want to see what all the data types are in a dataframe, we can use .dtypes method
df.info()     # Note: .info() returns a short summary of data farme including first and bottom 30 rows of the data frame.
df.describe() # Note: .describe() returns a each column's statistical summary such as count(number of terms), mean(average), std(standard deviation), min, 25% 50% 75% quartiles and max value
df.describe(include=all)  # Note:if we set the parameter include as all, .describe() methods return a full summary statistics sucg as unique(number of distinct objects), top(most frequently occuring objects), freq(the number of times the top object appears in the column)

# Accessing Databases with Python

# Python DB-API includes two concepts to access and query data from a database.
# 1- Connection objects provides data connections and managing transactions.
# 2- Cursor objects helps to conduct database queries.

# Connection methods
# .cursor() - returns a new cursor object using the connection
# .commit() - is used to commit any pending transaction to database
# .rollback() - causes the database to roll back to the start of any pending transaction
# .close() - is used to close a database connection


from dbmodule import connect

connection_object=connect("database_name", "user_name", "password")
cursor_object=connection_object.cursor()

cursor_object.execute("SELECT * from table_name")

results=cursor_object.fetchall()

cursor_object.close()
connection_object.close()

# Pre-processing Data in Python

# Data pre-processing is the process of converting or mapping data from the initial "raw" from into another format in order to prepare the data for further analysis. 
# It is also known as Data Cleaning or Data Wrangling.





# DEALING WITH MISSING VALUES IN PYTHON 

# Missing values occur when no data is stored for a variable(feature) in an observation. Missing values could be represented as "?", "N/A", "NaN", 0, or just a black cell.

# How to deal with missing data?
# 1- Check with the data collection source
#    The first is to check if the person or group that collected data can go back and find what the actual values shoul be.

# 2- Drop the missing values
#    Drop the variable or drop the data entry

# 3- Replace missing values
#    Replace it with an average value of similar data points
#    Replace it by frequency if the data type is an object not an integer or float.
#    Replace it based on other functions

# 4- Leave it as missing data


# Evaluating for Missing Data

# We can use .value_counts() method to see which values are present in a particular column.
df["column_name"].value_counts()

# We can also use the .idmax() method to calculate the most common value automatically
df["column_name"].value_counts().idxmax()



# To detect missing data, there are two methods which are .isnull() or .notnull() methods.
missing_data=df.isnull()  # Note: True means missing, False missing not missing.

# Counting missing values in each column
# We can use a for loop with .value_counts() method to count the number of True values with .isnull() method.
missing_data=df.isnull()
for column in missing_data.columns.values.tolist():       #Note: This syntax converts the data elements of an array into a list
  print(column)
  print(missing_data["column_name"].value_counts())



# How to drop missing values in Python
# We can use .dropna() method to drop missing values from a dataframe

df.dropna(axis=0) #Note: this line drops all rows that contain "NaN".
df.dropna(subset=["column_name"], axis=0)   # Note: axis=0 drops the entire row | axis=1 drops the entire column

# The syntax above does not change the dataframe driectly! We need to set inplace parameter to True to modify the dataset if we are sure!
df.dropna(subset=["column_name"], axis=0, inplace=True)

# We should reset indexes after dropping rows
df.reset_index(drop=True, inplace=True)





# DATA FORMATTING IN PYTHON (STANDARDIZATION)

# We need to look at the problem of different format, units and conventions, and the pandas methods that help us to deal with these issues.
# Bringing data into a common standard of expression allows users to make meaningful comparisons.

# Applying calculations to an entire column to convert unit
# Example to convert "mpg" to "L/100km" in a car dataset
df["city-mpg"]=235/df["cith-mpg"]
df.rename(columns={"city-mpg":"city-L/100km"}, inplace=True)

# Sometimes the wrong data type is assigned to a column. There are many data types in pandas.
# To identify data types, we can use .dtypes() method to check the data type of each variable.
# We use .astype() methods to convert data types
df["column_name"]=df["column_name"].astype("int")





# DATA NORMALIZATION IN PYTHON

# We need to uniform the features value with different range, so that we ontain simiar value ranges and similar intrinsic influence (asıl etki) on analytical model. 

# There are three approaches for normalization: 1- Simple Feture Scaling, 2- Min-Max, 3- Z-Score(Standard Score)

# 1- Simpe Feature Scaling:    X_new= X_current / X_max 
# the resulting value ranges between 0 and 1
df["column_name"]=df["column_name"] / df["column_name"].max()

# 2- Min-Max:                  X_new= (X_current-X_min) / (X_max-X_max)  Note: We should notice that X_max-X_max means the range of X values
# the resulting value ranges between 0 and 1
df["column_name"]=(df["column_name"]-df["column_name"].min())/(df["column_name"].max()-df["column_name"].min())

# 3- Z-Score (Standard Score): X_new=(X_current-Mu) / Sigma              Note: Mu is the average of the feature | Sigma is standard deviation.
# the resulting values are around zero and typicallay range between -3 and +3, but can be higher or lower.
df["column_name"]=(df["column_name"]-df["column_name"].mean())/df["column_name"].std()





# BINNING / DISCRETIZATION IN PYTHON

# Binning method is used to smoothing data or to handle noisy data. 
# In this method, the data is first sorted and then the sorted values are distributed into a number of buckets or bins. 
# Binning can be applied to convert numeric values to categorical or to sample (quantise) numeric values.


# If we want to have 3 bins of equal bindwith, we need 4 numbers as dividers that are equal distance apart.
# First, we use the numpy function .linspace() to return the array "bins" that contains 4 equally spaced numbers over the specified interval of space.
bins=np.space(min(df["price"]), max(df["price"]), 4)

# Second, we create a list "bin_names" that contains the different bin names.
bin_names=["Low", "Medium", "High"]

# Third, we use the pandas function .cut() to segment and sort the data values into bins.
df["price_binned"]=pd.cut(df["price"], bins, labels=bin_names, include_lowest=True)

# Finally, we can use histohrams to visualize the distribution of the data after they have been divided into bins.





# TURNING CATEGORICAL VARIABLES INTO QUANTITATIVE VARIABLES IN PYTHON 

# Most statistical models cannot take in objects or strings as input and for model training only take the number as inputs.
# The solution is One-hot encoding that adds dummy (sahte, boş) variables for each unique category and assign 0 or 1 for each category.

# In python, we use pandas .get_dummies() method to convert categorical variables to dummy variables (0 or 1).
pd.get_dummies(df["column_name"])

# EXPLORATORY DATA ANALYSIS (EDA)

# EDA is preliminary step in data analysis 
#      to summarize main characteristics of the data,
#      to gain beter understanding of the data set, 
#      to uncover relationships between variables, 
#      to extract important variables.

# The main question: What are the characteristics which have been the most impact on the target label?



# 1- Using Desciriptive Statistics in EDA

#   Descriptive statistics help to describe basic features of data and give short summaries about the sample and measures of the data.
#   We can use .describe() method to summarize statistics
df.describe()

# We can summarize the categorical data by using value_counts() methods
drive_wheels_counts:df["drive_wheels"].value_counts().to_frame                     # Note: .value_counts() returns the number of each value type | .to_frame() makes a table with results
drive_wheels_counts.rename(columns={"drive_wheels":"value_caunts"}, inplace=True)  # Note: .rename() is used to change the name of column in a dataset
drive_wheels_counts




# 2- Using Box Plots to visualize the various distirbution of data in EDA

#   By using a box plot, we can compare the groups in the data set. A box plot shows the upper extreme, upper quartile, median, lower quartile, lower extreme and outlier/single data point.
sns.boxplot(x="column_name1", y=column_name2, data=df)





# 3- Using Scatter Plots to visualize the relationship between to variables in EDA

#   In a scatter plot, each observation is represented as a point, and we can see the relationship between two data point to compare them.
#   X-axis generally represents predictor/independent variables.
#   Y-axis generally represents target/dependent variables
x=df["column_name1"]
y=df["column_name2"]

plt.scatter(x,y)
plt.title("Tiyle")
plt.xlabel("x_label")
plt.ylabel("y_label")




# 4- Using Groupby() method and Pivot Table in EDA

# .groupby() method is used on categorical variables, groups the data into sybsets according to different categories of that variable.
# We can group by a single variable or multiple variables by passing in multiple variable names.

df_test=df[["drive_wheels", "body_style", "price"]]   # Column names
df_group=df_test.groupby(["drive_wheels", "body_style"], as_index=False).mean()  # Note: Price column is groupped according to other two columns specified in .groupby() method
                                                                                 #      .mean() method adds means values of each group in the price column
df_group

# Pivot Table
# The result table will not be easy to understand, that is why we transform it to a pivot table by using .pivot() method.
# In a Pivot table, one varible is displayed as column names, and the other one is displayed as row indexes. The grouped data is displayed as the value in the table.
df_pivot=df_group.pivot(index="drive_wheels", columns="body_style")     # The price colums is written as the intersection values of these two columns to see groups easily.

# Another way to present a pivot table is a heat map plot.
# Heat map takes a rectangular grid of data and assigns a color intensity based on the data value at the grid points.
plt.pcolor(df_pivot, cmap="RdBu")
plt.colorbar()
plt.show()






# 5- Understanding correlations between continuous variables

# Correlation is a statistical metric for measuring to what extent different variables are interdependent.
# Correlation does not imply causation.
# we can calculate correlation between variables using .corr() method.
df.corr()

# We can display the correlation between variables by using a regression plot.
sns.regplot(x="engine_size", y="price", df)
plt.ylim(0,)

# If the slope of correlation line in the regression plot is positive, there is a positive correlation.
# If the slope of correlation line in the regression plot is negative, there is a negative correlation.
# If the slope of correlation line in regression plot is close to horizontal position, there is a weak correlation between variables and we cannot use this variable to predict target value.



# Pearson Correlation
# One way to measure the strength of the correlation between continuousnumerical variable is by using a method called Pearson Correlation.
# Pearson correlation method gives two values : 1- Correlation coefficient
#                                               2- P-value (Probability value)

# Correlation coefficient value is between -1 and +1.
#     Close to +1 means large positive relationship
#     Close to -1 means large negative relationship
#     Close to 0 means no relationship


# P-value is between 0 and 1. It does not imply existance of correlation, it proves the certainty of correlation result and your hypothesis. 
# P-value < 0.001 means Strong certainty in the correlation result.
# P-value < 0.05  means Moderate certainty in the correlation result.
# P-value < 0.1   means Weak certainty in the correlation results.
# P-value > 0.1   means No certainty in the correlation result.


# So for a strong correlation; correlation coefficient must be close to -1 or +1, and P-value must be less than 0.001
pearson_coef, p_value=stats.pearsonr(df["horsepower"], df["price"])    # Note: we use stats package of scipy python module.






# 6- Understanding association of two categorical variables : Chi-Square (donated as χ2)

# We cannot use the same correlation methods for continues variables to deal with the relationships between two categorical variables. 
# We have to employ the use of Chi-Square test for the association.

# The Chi-Square test is intended to test how likely it is that an observed distiribution is due to chance.
# It measures how well the observed distribution of data fits with the distribution that is expected if the variables are independent.

# The Chi-Square tests a null hypothesis that the variables are independent.
# Anytime the observed data does not fit within the model of the expected values, the probability that the variables are dependent becomes stronger, thus providing the null hypothesis incorrect.

# The Chi-Square does not tell you the type of the relationship that exists between both variables, but only that a relationship exists.
# It can also be used to test the goodness-of-fit between an observed distribution and a theoretical distribution of frequencies.

# χ2 depends on the size of the difference between actual and observed values, the degrees of freedom, and the sample size.



# First, we need to create a Contingency table (also called crosstab) which is used in statistics to summarise the relationship between several categorical variables.

# Second, We start by defining the null hypothesis (H0) which states that there is no relation between the variables. 
# An alternate hypothesis would state that there is a significant relation between the two. We can verify the hypothesis by these methods:

#       Using p-value: We define a significance factor to determine whether the relation between the variables is of considerable significance. 
#                      Generally a significance factor or alpha value of 0.05 is chosen. This alpha value denotes the probability of erroneously rejecting H0 when it is true. 
#                      A lower alpha value is chosen in cases where we expect more precision. 
#                      If the p-value for the test comes out to be strictly greater than the alpha value, then H0 holds true.

#       Using chi-square value: If our calculated value of chi-square is less or equal to the tabular(also called critical) value of chi-square, then H0 holds true.


# Third, Expected Values Table : we prepare a similar table of calculated (or expected) values. To do this we need to calculate each item in the new table as :
#                                Expected value(E)= (Row total x Colun Total)/Grand Total

# Fourth, Chi-Square Table : We prepare Chi-Square table by calculating for each item the following:
#                           (observed value-Calculated value)² / Calculated value


# Now, we need to find the critical value of chi-square. We can obtain this from a chi-sqaure table. 
# To use this table, we need to know the degrees of freedom for the dataset.  The degrees of freedom is defined as : (no. of rows – 1) * (no. of columns – 1).

# Then, we look at the table and find the value corresponding to 2 degrees of freedom and 0.05 significance factor :
#                         If the critical value of Chi-Square (χ2) on the table >= the calculated value of Chi-Square (χ2)
#                         Null hypothesis (H0) is accepted, that means the variables do not have a significant relation.



# In python
# The chi2_contingency() function of scipy.stats module takes as input, the contingency table in 2d array format. 
# It returns a tuple containing test statistics, the p-value, degrees of freedom and expected table(the one we created from the calculated values) in that order. 

from scipy.stats import chi2_contingency
  
# defining the table
data = [[207, 282, 241], [234, 242, 232]]
stat, p, dof, expected = chi2_contingency(data)
  
# interpret p-value
alpha = 0.05
print("p value is " + str(p))
if p <= alpha:
    print('Dependent (reject H0)')
else:
    print('Independent (H0 holds true)')


#Output will be: 
# p value is 0.1031971404730939
# Independent (H0 holds true)

# Since,
# p-value > alpha

"""### Model Development

---
"""

# A model or estimator can be thought of as mathematical equation used to predict the value given one or more other values.


## SIMPLE LINEAR REGRESSION ##

# Simple linear regression (SLR) is a method to help us understand the relationship between two variables.
#   1- The Predictor (Independent) Variable - x
#   2- The Target (Dependent) Variable - y

#   y=b0+b1.x     b0 - intercept (constant where the function crosses y-axis) |  b1 - slope

#   Generally we store data of x and y in a dataframe or array to fit the model and predict b0 and b1.

#   The uncertainty of model is taken into account by assuming a small random value is added to the point on the line. This is called noise, error or disturbance. 
#   Because of error (ε), all data points doesn't fit to regression line. Error is usually a small positive or negative value (close to zero), but sometimes, it can be a large value. 

from sklearn.linear_model import LinearRegression   # import linear_model module from scikit-learn

Lin_reg_model=LinearRegression()  # create a linear regression object using the constructor

x=df[["highway-mpg"]]             # we define the predictor variable and target variable
y=df["price"]

Lin_reg_model.fit(x,y)            # use .fit(independent variable, dependent variable) to fit the model, find the parameters b0 and b1
Lin_reg_model.intercept_          # use .intercept_ method to view the intercept (b0)
Lin_reg_model.coef_               # use .coef_ method to view the slope (b1)

Yhat=Lin_reg_model.predict(x)      # we can obtain a prediction using .predict(independent_variable) method



## MULTIPLE LINEAR REGRESSION ##

# This method is used to explain the relationship between one continuous target (y) variable and two or more predictor (x) variables.

# We aim to find the intercept (b0) and coefficients (b1, b2, b3......)  to estimate a linear regression model.
#   y = b0 + b1.x1 + b2.x2 + b3.x3.... 

# We can store predictor variables (independent variables) in an object.
z=df[["horsepower", "curb_weight", "engine_size", "highway-mpg"]]

Lin_reg_model.fit(z, df[price])   # we can train the model as before

Yhat=Lin_reg_model.predict(z)     # we can also obtain a prediction




## POLYNOMIAL REGRESSION AND PIPELINES ##

# We transfer our data into a ploynomial, then use linear regression to fit the parameter.
# Polynomial regression is a special case of the general linear regression model and it is useful for describing curvelinear relationships.

# Curvelinear relationships: by squaring or setting higher-order teram of the predictor variables.

# Quadratic 2nd order polynomial regression - y'=b0+b1x1+b2(x1)^2
# Cubic 3rd order polynomial regression - y'=b0+b1x1+b2(x1)^2+b3(x1)^3

f=np.polyfit(x,y,3)
p=np.poly1d(f)    # A one-dimensional polynomial class.
print(p)          # It prints a cubic function.




# Polynomial Regression with More Than One Dimension

# Two dimensional 2nd order polynomial
y'=b0 + b1.x1 +b2.x2 + b3.x1.x2 + b4.(x1)**2 + b5.(x2)**2+....

# Numpy's polyfit function cannot perform this type of regression. That's why we use the pre-processing library scikit-learn to create a polynomial object.
sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=2 include_bias=False)              # Train the object
df_polly=pr.fit_transform(df[["horsepower", "curb-weight"]])

# As the dimension of data get larger, we may want to normalize multiple features in scikit-learn, and we can use the pre-processing module to simplify many tasks.
# We can normalize the each feature simultaneously.
from sklearn.preprocessing import StandardScaler
scale=StandardScaler()                                            # Train the object
scale.fit(x_data[["horsepower", "highway-mpg"]])                  # fit the scale object
x_scale=scale.transform(x_data[["horsepower", "highway-mpg"]])    # transform data into a new data frame


# PIPELINE
# We can simplify our code by using a pipeline library
# Pipeline sequentially perform a series of transformation. The last step carries out a prediction. An examply of pipeline steps: Normalization - Polynomial Transform - Linear Regression

from sklearn.preprocessing import PolynomialFeatures   # First, we need to import needed modules for a pipeline
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

Input=[("scale", StandardScaler()),("polynomial",PolynomialFeatures(degree=2)), ("mode",LinearRegression())]  # Second, we define an object with name of estimatores and their corresponding constructors

pipe=Pipeline(Input)    # Third, we define a pipe object by using Pipeline() constructor.

pipe.fit(df[["horsepower", "curb_weight", "engine_size", "highway-mpg"]])           # Fourth, we train the pipeline object.

Yhat=pipe.predict(x[["horsepower", "curb_weight", "engine_size", "highway-mpg"]])   # Fifth, we produce a prediction.






## IN-SAMPLE MODEL EVALUATION USING VISUALIZATION

# 1- Regression Plot
# Regression plots give a good estimation of:
#                                             The relationship between two variable
#                                             The strength of correlation
#                                             The direction of the relationship (positive or negative)

# Regression plots show us a combination of the scatter plot (where each point represents a different y) and the fitted linear regression line (y')

# We can use seaborn library to plot a regression plot
import seaborn as sns
sns.regplot(x="high-mpg", y="price", data=df)
plt.ylim(0,)



# 2- Residual Plot
# Residual Plots represent the error between the actual and predicted values
# y'-y= Differences between predicted and actual values of y for the same x points are represented in the residual plot as points. 
# We expect to see the results have zer(0) mean and distributed evenly around the x-axis with similar variance and there is no curvature.
import seaborn as sns
sns.residplot(df["dependent_variable", "target_variable"])



# 3- Distribution Plot
# This type of plot is extremely useful for visualizing models with more than one independent variable or feature.
# The vertical axis is scaled to make the area under the distribution equal to one.

import seaborn sns

ax1=sns.displot(df["price"], hist=False, color="r", label="Actual Value")
    sns.displot(Yhat, hist=False, color="b", label="Fitted Values", ax=ax1)




## IN-SAMPLE MODEL EVALUATION FOR IN-SAMPLE EVALUATION USING NUMERIC MEASURES

# Instead of visualization, we can numerically evaluate our models. These measures are a way to numerically determine how good the model fits on our data.
# Two important measures to determine the fit of a model:
#                                                       1. Mean Squared Error (MSE)
#                                                       2. R-Squared (R^2 or R**2)

# 1. Mean Squared Error (MSE)
# To measure the MSE, we find the difference between the actual value y and predicted value y', then square it. 
# Gerçek ve tahmin edilen değerlerin farkının karesi alınır ve toplanır. Ortalamasını almak için de tahmin sayısına bölünür. MSE büyüdükçe fitlik azalabilir.

# Mean Squared Error= ((Y0-Y0')**2 + (Y1-Y1')**2 + (Y2-Y2')**2 + .......) / Number of samples      

from sklearn.metrics import mean_squared_error
mean_squared_error(df["price"], y_predict_simple_fit)     # df["price"] = actual values , y_predict_simple_fit = predicted values

# Does a lower mean squared error imply better fit?
# Not necessarily!
# 1-MSE for a multiple linear regression model will be smaller than the MSE for a simple linear regression model since the error of the data will decrease when more variables are included in the model!
# 2-Polynomial regression will also have a similar MSE than the linear regular regression.



# 2. R-Squared (R^2 or R**2)
# The coefficient of determination or r-squared (R^2) is a measure to determine how close the data is to fitted regression line.
# R^2: the percentage of variation of the target variable (y), that is explained by the linear model.
# Think about as comparing a regression model to a simple model i.e. the mean of data points. If the variable x is a good predictor, our predictor should perform much better than mean.

# R**2=(1 - (MSE of regression line/MSE of the average of the data)     Note: The result is between 0 and 1. The model explains the 100% variations of predicted data if the result is 1.
#                                                                                                            So the closer value to 1 means more accurate prediction.
#                                                                                                            If the result is minus, the model is over fitting!!!!!!
 
x=df[["highway-mpg"]]
y=df["price"]

lm.fit(x,y)
lm.score(x,y) # it provides the valur of R**2


# Precition and Decision Making
# To determine the final best fit, we look at a combination of:
#                                                              - make sure your model results make sense
#                                                              - use visualization
#                                                              - numerical measures for evaluation
#                                                              - comparing models





## OUT-OF-SAMPLE MODEL EVALUATION AND REFINEMENT

# In-sample model evaluation tells us how well our model will fit the data used to train it. However, it does not tell us how well the trained model can be used to predict new data.
# The solution is to split our data up, use the in-simple data or training data to train the model. The rest of the data, called Test Data, is used as out-of-sample data.
# Test data is used to approximate how the model performs in real world.

# When we split a dataset, usually larger portion of data is used for training and a small part is used for testing.
#   1-Split dataset into training set(70%) and testing set(30%)
#   2- Build and train the model with training set
#   3- Use testing set to assess the performance of predictive model
#   4- When we have completed testing our model, we should use all the data to train the model to get the best performance.

# train_test_split() function is apopular function in scilit-learn package for splitting datasets. This function randomly splits a dataset into training and testing subsets.

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x_data, y_data, test_size=0.3, ramdom_state=0)

# x_data - features or independent variables
# y_data - target variable

# x_train, y_train - parts of avaible data as training set (array)
# x_test, y_test - parts of avaible data as testing set (array)

# test_size - percentage of the data for testing (it is 30% in the example)
# random_state - number of generator used for random sampling (random seed)



# Generalization Performance and Cross Validation

# Generalization error is the measure of how well our data does at predicting unseen data. The error we obtain using our testing data is an approximation of this error.
# If we train our model with random training datasets, we obtain different results with different errors for each time, and that cause generalization error. 

# To habdle generalization error, the most common out-of sample evaluation metric is cross-validation.
#   1- The dataset is split into K equal groups and each group is referred as a fold.
#   2- Some of folds are used as traning dataset, remaining folds are used as testing dataset.
#   3- Each fold is used for both traning and testing by repeating the model development process.
#   4- At the end, the average of result are used to estimate out-uf-sample error.

from sklearn.mdel_selection import cross_val_score
scores=cross_val_score(lr, x_data, y_data, cv=3)    # lr - type of model(linear regression), cv - number of folds
np.mean(scores)                                     # out-of-sample error of model as R**2

# If we want to know predicted values for each selected testing tests, we use cross_val_predict() function.
from sklearn.model_selection import cross_val_predict
yhat=cross_val_predict(lr, x_data, y_data, cv=3)       # The output is prediction arrays





## OVERFITTING, UNDERFITTING AND MODEL SELECTION ##

# The goal of model selection is is to determine the order of polynomial to provide the best estimate of function.

# If we select a poor polynomial function, the output line is not complex to fit the data. This is underfitting.
# If we select the polynomial function with the right order, it fits well the data.
# If we select extremely high order of polynomial, the predicted curve follows data points instead of function. This is apparent where there is little training data. This is called overfitting.

# If we select the best order of the polynomial, we still have error, and one reason for this is noise because it is random and we cannot predict it. This is sometimes referred as irreducible error.
# There are other sources of errors. For example, the polynomial assumption may be wrong. For real data, the model may be too difficult to fit or we may not have the correct type of data to estimate the function.

# We can check different R**2 values to see the results for different orders of polynomials. In python,
Rsqu_test=[]      # empty list to store the values
order=[1,2,3,4]   # list of polynomial orders

for n in order:
  pr=PolynomialFeatures(degree=n)                       # We create a polynomial feature object
  x_train_pr=pr.fit_transform(x_train[["horsepower"]])  # We transform the training and test data into a polynomial
  x_test_pr=pr.fit_transform(x_test[["horsepower"]])    # We transform the training and test data into a polynomial
  lr.fit(x_train_pr, y_train)                           # We fit the regression model

  Rsqu_test.append(lr.score(x_test_pr, y_test))         # We calculate R**2 values and store it in the array







## RIDGE REGRESSION FOR MULTICOLLINEARITY (ÇOKLU KORELASYON)

# Ridge regression is a regression that is employed in a multiple regression model when multicollinearity (çoklu korelasyon) occurs. 
# Multicollinearity is when there are strong relationships among the independent variables. Ridge regression is very common with polynomial regression.

# Ridge regression controls the magnitude of polynomial coefficients by introducing the parameter Alpha.
# Alpha is a parameter we select before fitting or training the model.

from sklearn.linear_model import Ridge
RidgeModel=Ridge(alpha=0.1)
RidgeModel.fit(x,y)
yhat=RidgeModel.predict(x)





## GRID SEARCH METHOD TO OPTIMIZE PARAMETERS OF MACHINE LEARNING MODELS ##

# We call the parameters that a model can not learn by itself as hyperparameter such as Alpha.
# Sckit-learn has means of automatically iterating over hyperparameters using cross-validation and this is called Grid Search.
# Grid Search takes the model or objects you would like to train and different values of hyperparameters. It then calculates the mean square error or R-squared for various hyperparamaters.

# 1- We start off with one value for hyperparameters and train the model.
# 2- Next, we use different hyperparameters to train the model.
# 3- We continue until we have exhausted the different parameter values.
# 4- Consequently, each model produces an error and we select the hyperparameter that minimizes the error.



# How to select hyperparameter?
#   1- Split our dataset into three parts, the training set, the validation set and test set.
#   2- We train the model for different hyperparameters.
#   3- We use R-squared or mean squared error for each model.
#   4- We select the hyperparameter that minimizes the mean squared error or maximizes the R-quared eror on validation test.
#   5- We finally test our model performance using the test data.

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

parameters=[{"alpha":[0.001,0.1,1,1,10,100,1000,10000,100000,1000000,]}]      # hyperparameters
RR=Ridge()

Grid1=GridSearchCV(RR, parameters, cv=4)  # we did not define a scoring method because it is R-squared as default
Grid1.fix(x_data[["horsepower", "curbweight", "engine_size", "Highway-mpg"]], y_data) 
Grid1.best_estimator    # we can find the best values for the hyperparameters                                             

scores=Grid1.cv_results_    # we can get the information like the mean score on the validation data
scores["mean_test_score"]



# Ridge regression has the option to normalize the data and we can also use it as a hyperparameter.
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

parameters=[{"alpha":[0.001,0.1,1,10,100], "normalize":[True, False]}]
RR=Ridge()

Grid1=GridSearchCV(RR, parameters, cv=4)
Grid1=fix(x_data[["horsepower", "curbweight", "engine_size", "highway-mpg"]], y_data)
Grid1.best_estimator_
scores=Grid1.cv_results_

for param, mean_val, mean_test in zip(scores["params", scores["mean_test_score"], scores["mean_train_score"]]):   # We can print out the score for the different hypeparameters.
  print(param, "R^2 on test data:", mean_val, "R^2 on train data", mean_test)

"""### Data Visulization with Python

---
"""

# When creating a visual presentation, always remember; Less is more effective - Less is more attractive - Less is more impactive

# Matplotlib's archiecture is compsed of three layers: 1- Backend layer (It also has three layers which are FigureCanvas, Renderer and Events), 2- Artist layer, , 3- Script layer (pyplot)

# 1- Backend Layer

# This layer has three built-in abstract interface classes.
#       1- FigureCanvas: matplotlib.backend_basis.FigureCanvas
#                        It encompasesses the area onto which the figure is drawn
#       2- Renderer:     matplotlib.backend_basis.Renderer
#                        It knows how to drawon the FigureCanvas
#       3- Events:       matplotlib.backend_basis.Event
#                        It handles user inputs such as keyboard strokes and mouse clicks

# 2- Artist Layer

# This layer was comprised of one main object, Artist who knows how to use the Renderer to draw on the canvas.
# Title, lines, tick labels, and images, all correspond to individual Artist instances.
# There are two types of Artist object:
#                                     1- Primitive: Line2D, Rectangle, Circle and Text
#                                     2- Composite: Axis, Tick and Figure

# Each composite artist object may contain other composite artist object as well as primitives.

from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from matplotlib.figure import Figure
Fig=Figure()
canvas=FigureCanvas(Fig)

# 3- Scripting Layer (pyplot)

# This layer was developed for scientists who are not professional programmers because the artist layer is heavy as it is meant for developers.
# This layer is mainly composed of pyplot, a scripting interface that is lighter than artist layer.

import matplotlib.pyplot as plt
import numpy as np

x=np.random.randn(10000)

plt.hist(x,100)
plt.title(r"random analysis")
plt.savefig("matplotlib_plt_histogram.png")
plt.show()

# LINE PLOTS

# A line plot is a type of plot which display information as a series of data points called "markers" connected by straight line segments.
# The best use case for line plot is when you have a continuous dataset and you are interested in visualizing the data over a period of time.
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

years=list(map(str,range(1980,2014)))
df.canada.loc["Haiti", years].plot(kind="line")

plt.title("Immigration from Haiti")
plt.xlable("years")
plt.ylable("Numbers of Immigrants")
plt.show()

# AREA PLOTS

# Area plot, also known as area chart, is commonly used to represent cumulated totals using numbers or percentages ove time and it is based on lin eplot.

years=list(map(str,range(1980,2014)))
df_canada.sort_values(["Total"], ascending=False, axis=0, inplace=True)

df_top5=df_canada.head()
df_top5=df_top5[years].transpose()  # It changes columns name and row index numbers with each other to revise the data frame.

import matplotlib as mpl
import matplotlib.pyplot as plt

df_top5.plot(lind="area")

plt.title("Immigration from Haiti")
plt.xlable("years")
plt.ylable("Numbers of Immigrants")
plt.show()

# HISTOGRAM

# A histogram is a way of representinh the frequrncy distribution of a variable.
# We divide data into bins to define equal distribution over the histogram, so we need to use numpy.
# Next, we draw bars of that height for the corresponding each bins to have an histogram.

import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

count, bin_edges=np.histogram(df_canada["2013"])      # count - for the frequency of each bin |  bin_edges - to define the limit edges of each bin  | np.histogram() function in numpy

df_canada["2013"].plot(kind="hist", xsticks=bin_edges)

plt.title("...................")
plt.xlable("..................")
plt.ylable("..................")
plt.show()

# BAR CHART

# Unlike a histogram, a bar chart is commonly used to compare the values of a variable at a given point in time.

import matplotlib as mpl
import matplotlib.pyplot as plt

years=list(map(str, range(1980, 2014)))
df_iceland=df_canada.loc["Iceland", years]
df_iceland.plot(kind="bar")
plt.title("...................")
plt.xlable("..................")
plt.ylable("..................")
plt.show()

# PIE CHART
import matplotlib as mpl
import matplotlib.pyplot as plt

df_continents=df_canada.groupby("continent", axis=0).sum()
df_continents["Total"].plot(kind="pie")
plt.title("...................")
plt.show()

# BOX PLOT

# A box plot is a way of statistically representing the distribution of given data through 5 main dimensions.
# Minimum
# First quartile
# Median
# Third quartile
# Maximum

# Box plots also display outliers as individual dots that occur outside the upper and lower extremes.

import matplotlib as mpl
import matplotlib.pyplot as plt

df_japan=df-canada[["japan"], years].transpose()

df_japan.plot(kind="box")
plt.title("...................")
plt.ylable("..................")    # Only y label
plt.show()

# SCATTER PLOT 

import matplotlib as mpl
import matplotlib.pyplot as plt

df_total.plot(kind="scatter", x="year", y="total")
plt.title("...................")
plt.xlable("..................")
plt.ylable("..................")
plt.show()

# WAFFLE CHARTS

# A Waffle chart is a way to visualize data in relation to a whole or to highlight progress against a given threshold.
# Matplotlib does not have a built-in function to create waffle charts. So, we need to create our own python function to create a waffle chart.

# SEABORN AND REGRESSION PLOTS

# Seaborn is a Python visualization library based on matplotlib.
# Visuals that need more than 20 lines of codes using Matplotlib to be created, with seaborn, the number of lines of code is reduced by 5-fold.

import seaborn as sns
ax=sns.regplot(x="year", y="total", data=df_total, color="green", marker="+")

# VISUALIZING GEOSPATIAL DATA

# Folium is a powerful python library that helps you create several types of leaflet maps. With folium, we can create a map of any location in the world as long as we know its latitude and longtitude values.
# It enables both binding of data to a map for choropleth visualizations as well as passing visualizations as markers on the map.
# The library has a number of built-in tilesets from OpenStreetMap, Mapbox and Stamen, and supports custom tilesets with mapbox API keys.


# Folium creates an interactive world map on which you can zoom in and out. The default map style is the open street view of an area when you are zoomed .n and shows the borders of the world countries.
!pip install folium
import folium

world_map=folium.Map() # define the world map
world_map              # display the map


# Creating map of Canada
canada_on_world_map=folium.Map(location=[56.130,-106.35], zoom_start=4)
canada_on_world_map


# Another amazing feature of folium is that we can create different map styles using the tiles parameter.
turkey_on_world_map=folium.Map(location=[38.9637,35.2433], zoom_start=6, tiles="Stamen Toner")
turkey_on_world_map



# Maps with MARKERS
turkey_on_world_map=folium.Map(location=[38.9637,35.2433], zoom_start=6, tiles="Stamen Toner")      # Generate map of Turkey

malatya=folium.map.FeatureGroup()    # create an empty feature group
malatya.add_child(folium.features.CircleMarker([38.356869, 38.309669], radius=10, color="red", fill_color="red"))   # define the style of feature gorup

turkey_on_world_map.add_child(malatya)  # add the defined feature group on map

folium.Marker([38.356869, 38.309669], popup="Malatya").add_to(turkey_on_world_map)  # Lastly, if we want, we can add a location marker with its label to display when we click on it

turkey_on_world_map





# Choropleth Maps

# A choropleth map is a thematic map in which areas are shaded or patterred in proportion to the measurement of the statistical variable being displayed on the map, such as population density or per capita income. 
# The higher the measurement the darker the color.

# For a choropleth map of the world, we would need a Geojson file that includes data of the region. We would need a geojson file that lists each country along with any geospatial data to define its borders and boundaries.

world_map=folium.Map(zoom_start=2, tiles="Mapbox Bright") # create a plain world map

world_geo=r'world_countries.json' # geojson file

# generate choropleth map using the total population of each country to Canada from 1980 to 2013
world_map.choropleth(geo_path=world_geo, data=df_canada, columns=["Country", "Total"], key_on="feature.properties.name", fill_color="YlOrRd", legend_name="Immigratio to Canada")

world_map

## DASHBOARD ##

# A Dashboad:
#            shows real-time visuals
#            understand business moving parts
#            visually track, analyze and display key performance indicators (KPI)
#            take imformed decisions and improve performance
#            reduce hours of analyzing
# Best dashboards answer important business questions.


## Web-based Dashboarding ##

# Dash from Plotly is a python framework for building web analytic applications. It is written on top of flask, plotly.js and react.js. Dash is well-suited for building data visualization apps with highly custom user interfaces.

# Panel works with visualizationsfrom Bokeh, Matplotlib, HoloViews, and many other python plotting libraries, making them instantly viewable either individually or when combined with interactive widgets that control them

# Voila turns Jupyter notebooks into standalone web applications. It can be used with seperate layout tools like jupyter-flex or templates like voila-vuetify.

# Streamlit can easily turn data scripts into sharable web apps with 3 main principles; embrace python scripting, treat widgets as variables, and reuse data and computations.

# There are oyher tools that can be used for dashboarding: Bokeh, ipywidgets, matplotlib, Bowtie, Flask



# Plotly and Dash#
# Plotly is an interactive, open source plotting that supports over 40 unique chart types. It is avaible in python, R, javascript. 
# Plotly python is built on top of plotly javascript library and includes chart types like statistical, financial, maps, scientific, and 3-dimensional data.
# The web-based visualizations created using plotly python can be displayed in Jupyter notebook, saved to stand-alone HTML files, or served as part of pure python-built web applications using Dash.

#There are two types of Plotly sub-moduyles:
#                            1. Plotly Graph Objects: are low-level interface to figure, trace and layout. The module provides an automatically generated hierarchy of classes (figures, layouts) called graph objects.
#                            2. Plotly Express: is ahigh-level wrapper for Plotly. It is recommended starting point for creating most common figures provided by Plotly usinf a more simple synatx. It uses graph objects internally.


# Using plotly.graph_objects
import plotly.graph_objects as go     # import required packages
import plotly.express as px
import numpy as np

np.random.seed(10)    # set random seed for reproducibility
x=np.arange(12)

y=np.random.randint(50,500,size=12)   # create random y values

# Line Plot using graph object
# Plotly.graph contains a JSON object which has a structure of dict.
# Here, "go" is the plotly JSON object
# Updating values of "go" object keywords, chart can be plotted.
# Create figure and add trave (scatter)

fig=go.Figure(data=go.Scatter(x=x, y=y)
fig.update_layout(title="Simple Line Plot", xaxis_title="Month", yaxis_title="Sales")
fig.show()


# We can generate the same graph generated with graph object by using plotly.express                ##### Website: https://plotly.com/python/plotly-express/
fig=px.line(x=x, y=y, title="Simple Line Plot", labels=dic(x="Month", y="Sales"))
fig.show()


# Dash
# Dash is a open-source user interface python library from plotly for creating reactive, web-based applications. It is enterprise-ready and a first-class member of Plotly's open source tools.
# Dash applications are web servers running Flask and communicating JSON packets over http requests.
# Dash's frontend render components using React.js. It is easy to build a Graphical User Interface (GUI) using dash.
# Dash is declarative and reactive.
# Dash output can be rendered in a web browser and can be deployed to servers.
# Dash uses a simple reactive decorator for binding code to the UI. This is inherently mobile and cross-platform ready.

# As a first step, you need to determine the layout of the application. Decide which chart to use and where to place. This called "layput" part in dash.
# The second part is to add interactivity to the application.

# There are two components of Dash:
#                                  - Core components  - import dash_core_components as dcc
#                                  - Html components  - import dash_html_components as html

# The dash_core_components describe higher level components that are interactive and generated with Javascript, html and css through the React.js library. Some example of core 






# Make dahboard interactive
# callback function is a python function that is automatically called by Dash whenever an input component's property changes.
# calback function is decorated with "@app.callback" decorator
@app.callback(Output, Input)
defcallback__function:
  .........
  .........
  return some_result

# DASHBOARD APPLICATION

# TASK 1 - Data Preparation

# Let's start with importing required packages
import pandas as pd
import plotly.express as px
import dash
import dash_html_components as html
import dash_core_components as dcc

# The ‘dash’ contains the app which will be run on the browser and involves the layout of the app (dashboard).
# The ‘dash_core_components’ contains the higher level components like the graphs, dropdown, tabs, etc. ‘dcc’ makes the dashboard a lot more interactive.
# And all the html components such as div, img, h1, h2 and so on are included in ‘dash_html_components’. ‘html’ lets you to customize your dashboard and make it pretty and comprehendible.
# We use ‘as’ to give a shorter and easier alias.






# Read the airline data into pandas dataframe
airline_data =  pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DV0101EN-SkillsNetwork/Data%20Files/airline_data.csv', 
                            encoding = "ISO-8859-1",
                            dtype={'Div1Airport': str, 'Div1TailNum': str, 
                                   'Div2Airport': str, 'Div2TailNum': str})

# Randomly sample 500 data points. Setting the random state to be 42 so that we get same result.
data = airline_data.sample(n=500, random_state=42)

# Pie Chart Creation
fig = px.pie(data, values='Flights', names='DistanceGroup', title='Distance group proportion by flights')





# TASK 2 - Create dash application and get the layout skeleton

# Create a dash application
app = dash.Dash(__name__)    # Next step is to initialize the app by creating an instance. Your dash app is now ready to be designed.




# Get the layout of the application and adjust it. This is where we align the dashboard. This is quite similar to designing an HTML website. You embed a whole bunch of divs in one parent div and align them as you like.
# Create an outer division using html.Div and add title to the dashboard using html.H1 component
# Add description about the graph using HTML P (paragraph) component
# Finally, add graph component.

app.layout = html.Div(children=[html.H1(),            # The app.layout stores the entire layout of your dashboard. This is where you pass the parent div containing different divs.
                                html.P(),             # The html.Div() has many parameters like ‘id’, ‘className’, ‘style’, ‘children’ and others may vary accordingly.
                                dcc.Graph(),          # there are 3 different ways to apply css. This is one method and is called as inline css. 
                    ])                                # You can also use external css file. You need to create a folder named as ‘assets’ and create a .css file inside it which will be applied to your app. You can name the css file by any name you want. The classname helps you customize your dashboard when you use external css.
                                
                                
# Mapping to the respective Dash HTML tags:
#                                         Title added using html.H1() tag
#                                         Description added using html.P() tag
#                                         Chart added using dcc.Graph() tag



# Run the application                   
if __name__ == '__main__':
    app.run_server()






# TASK 3 - Add the application title
# Update the html.H1() tag to hold the application title.

# Application title is Airline Dashboard
# Use style parameter provided below to make the title center aligned, with color code #503D36, and font-size as 40

app.layout = html.Div(children=[html.H1('Airline Dashboard',style={'textAlign': 'center', 'color': '#503D36', 'font-size': 40}),
                                html.P(),
                                dcc.Graph(),
                    ])

# Run the application                   
if __name__ == '__main__':
    app.run_server()







# TASK 4 - Add the application description
# Update the html.P() tag to hold the description of the application.

# Description is Proportion of distance group (250 mile distance interval group) by flights.
# Use style parameter to make the description center aligned and with color #F57241.

app.layout = html.Div(children=[html.H1('Airline Dashboard',style={'textAlign': 'center', 'color': '#503D36', 'font-size': 40}),
                                html.P(('Proportion of distance group (250 mile distance interval group) by flights.', style={'textAlign':'center', 'color': '#F57241'}),),
                                dcc.Graph(),
                    ])

# Run the application                   
if __name__ == '__main__':
    app.run_server()








# TASK 5 - Update the graph
# Update figure parameter of dcc.Graph() component to add the pie chart. We have created pie chart and assigned it to fig. Let's use that to update the figure parameter.

app.layout = html.Div(children=[html.H1('Airline Dashboard',style={'textAlign': 'center', 'color': '#503D36', 'font-size': 40}),
                                html.P(('Proportion of distance group (250 mile distance interval group) by flights.', style={'textAlign':'center', 'color': '#F57241'}),),
                                dcc.Graph((figure=fig)),
                    ])

# Run the application                   
if __name__ == '__main__':
    app.run_server()





# TASK 6 - Run the application
# Run the python file using the following command in the terminal
python3 dash_basics.py

# Press ctrl+c inside the terminal to stop the dash application.

#  DASBOARD RECAB

!pip install pandas dash
!pip3 install httpx==0.20 dash plotly

# Import required packages
import pandas as pd
import plotly.express as px
import dash
import dash_html_components as html
import dash_core_components as dcc

# Read the airline data into pandas dataframe
airline_data =  pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DV0101EN-SkillsNetwork/Data%20Files/airline_data.csv', 
                            encoding = "ISO-8859-1",
                            dtype={'Div1Airport': str, 'Div1TailNum': str, 
                                   'Div2Airport': str, 'Div2TailNum': str})

# Randomly sample 500 data points. Setting the random state to be 42 so that we get same result.
data = airline_data.sample(n=500, random_state=42)

# Pie Chart Creation
fig = px.pie(data, values='Flights', names='DistanceGroup', title='Distance group proportion by flights')

# Create a dash application
app = dash.Dash(__name__)

# Get the layout of the application and adjust it.
# Create an outer division using html.Div and add title to the dashboard using html.H1 component
# Add description about the graph using HTML P (paragraph) component
# Finally, add graph component.
app.layout = html.Div(children=[html.H1('Airline Dashboard', style={'textAlign': 'center', 'color': '#503D36', 'font-size': 40}),
                                html.P('Proportion of distance group (250 mile distance interval group) by flights.', style={'textAlign':'center', 'color': '#F57241'}),
                                dcc.Graph(figure=fig),

                    ])

# Run the application                   
if __name__ == '__main__':
    app.run_server()

from scipy.stats.distributions import TRUNCNORM_MAX_BRENT_ITERS

#  DASBOARD INTERACTIVE - RECAB

!pip install pandas dash
!pip3 install httpx==0.20 dash plotly


# Import required libraries
import pandas as pd
import plotly.graph_objects as go
import dash
import dash_html_components as html
import dash_core_components as dcc
from dash.dependencies import Input, Output

# Read the airline data into pandas dataframe
airline_data =  pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DV0101EN-SkillsNetwork/Data%20Files/airline_data.csv', 
                            encoding = "ISO-8859-1",
                            dtype={'Div1Airport': str, 'Div1TailNum': str, 
                                   'Div2Airport': str, 'Div2TailNum': str})
# Create a dash application
app = dash.Dash(__name__)

app.layout = html.Div(children=[ html.H1('Airline Performance Dashboard',style={'textAlign': 'center', 'color': '#503D36','font-size': 40}),
                                html.Div(["Input Year: ", dcc.Input(id='input-year', value='2010', 
                                type='number', style={'height':'50px', 'font-size': 35}),], 
                                style={'font-size': 40}),
                                html.Br(),
                                html.Br(),
                                html.Div(dcc.Graph(id='line-plot')),
                                ])

# add callback decorator
@app.callback( Output(component_id='line-plot', component_property='figure'),
               Input(component_id='input-year', component_property='value'))

# Add computation to callback function and return graph
def get_graph(entered_year):
    # Select 2019 data
    df =  airline_data[airline_data['Year']==int(entered_year)]

    # Group the data by Month and compute average over arrival delay time.
    line_data = df.groupby('Month')['ArrDelay'].mean().reset_index()

    fig = go.Figure(data=go.Scatter(x=line_data['Month'], y=line_data['ArrDelay'], mode='lines', marker=dict(color='green')))
    fig.update_layout(title='Month vs Average Flight Delay Time', xaxis_title='Month', yaxis_title='ArrDelay')
    return fig

# Run the app
if __name__ == '__main__':
    app.run_server(port=8002, host='127.0.0.1', debug=TRUNCNORM_MAX_BRENT_ITERS)

# We can run the above code in the terminal by using following steps. 

# TASK 6 - Run the application
# Firstly, install pandas and dash using the following command in the terminal
pip3 install pandas dash

# Copy and paste the below command in the terminal to run the application.
python3 dash_interactivity.py

# Observe the port number shown in the terminal.
# Click on the Launch Application option from the side menu bar.Provide the port number and click OK


#The app will open in a new browser tab

## US Domestic Airline Flights Performance Dashboard

# Import required libraries
import pandas as pd
import dash
import dash_html_components as html
import dash_core_components as dcc
from dash.dependencies import Input, Output, State
import plotly.graph_objects as go
import plotly.express as px
from dash import no_update


# Create a dash application
app = dash.Dash(__name__)

# REVIEW1: Clear the layout and do not display exception till callback gets executed
app.config.suppress_callback_exceptions = True

# Read the airline data into pandas dataframe
airline_data =  pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DV0101EN-SkillsNetwork/Data%20Files/airline_data.csv', 
                            encoding = "ISO-8859-1",
                            dtype={'Div1Airport': str, 'Div1TailNum': str, 
                                   'Div2Airport': str, 'Div2TailNum': str})


# List of years 
year_list = [i for i in range(2005, 2021, 1)]

"""Compute graph data for creating yearly airline performance report 

Function that takes airline data as input and create 5 dataframes based on the grouping condition to be used for plottling charts and grphs.

Argument:
     
    df: Filtered dataframe
    
Returns:
   Dataframes to create graph. 
"""

def compute_data_choice_1(df):
    # Cancellation Category Count
    bar_data = df.groupby(['Month','CancellationCode'])['Flights'].sum().reset_index()
    # Average flight time by reporting airline
    line_data = df.groupby(['Month','Reporting_Airline'])['AirTime'].mean().reset_index()
    # Diverted Airport Landings
    div_data = df[df['DivAirportLandings'] != 0.0]
    # Source state count
    map_data = df.groupby(['OriginState'])['Flights'].sum().reset_index()
    # Destination state count
    tree_data = df.groupby(['DestState', 'Reporting_Airline'])['Flights'].sum().reset_index()
    return bar_data, line_data, div_data, map_data, tree_data


"""Compute graph data for creating yearly airline delay report

This function takes in airline data and selected year as an input and performs computation for creating charts and plots.

Arguments:
    df: Input airline data.
    
Returns:
    Computed average dataframes for carrier delay, weather delay, NAS delay, security delay, and late aircraft delay.
"""
def compute_data_choice_2(df):
    # Compute delay averages
    avg_car = df.groupby(['Month','Reporting_Airline'])['CarrierDelay'].mean().reset_index()
    avg_weather = df.groupby(['Month','Reporting_Airline'])['WeatherDelay'].mean().reset_index()
    avg_NAS = df.groupby(['Month','Reporting_Airline'])['NASDelay'].mean().reset_index()
    avg_sec = df.groupby(['Month','Reporting_Airline'])['SecurityDelay'].mean().reset_index()
    avg_late = df.groupby(['Month','Reporting_Airline'])['LateAircraftDelay'].mean().reset_index()
    return avg_car, avg_weather, avg_NAS, avg_sec, avg_late

# Application layout
app.layout = html.Div(children=[ 
                                # TASK1: Add title to the dashboard
                                # Enter your code below. Make sure you have correct formatting.
                                html.H1('US Domestic Airline Flights Performance',style={'textAlign': 'center', 'color': '#503D36', 'font-size': 24}),
                                # REVIEW2: Dropdown creation
                                # Create an outer division 
                                html.Div([
                                    # Add an division
                                    html.Div([
                                        # Create an division for adding dropdown helper text for report type
                                        html.Div(
                                            [
                                            html.H2('Report Type:', style={'margin-right': '2em'}),
                                            ]
                                        ),
                                        # TASK2: Add a dropdown
                                        # Enter your code below. Make sure you have correct formatting.
                                        dcc.Dropdown(id='input-type', 
                                                    options=[{'label':'Yearly Airline Performance Report', 'value':'OPT1'}, 
                                                             {'label':'Yearly Airline Delay Report', 'value':'OPT2'}], 
                                                    placeholder='Select a report type', 
                                                    style={'width':'80%', 'padding':'3px', 'font-size': '20px', 'text-align-last' : 'center'})
                                    # Place them next to each other using the division style
                                    ], style={'display':'flex'}),
                                    
                                   # Add next division 
                                   html.Div([
                                       # Create an division for adding dropdown helper text for choosing year
                                        html.Div(
                                            [
                                            html.H2('Choose Year:', style={'margin-right': '2em'})
                                            ]
                                        ),
                                        dcc.Dropdown(id='input-year', 
                                                     # Update dropdown values using list comphrehension
                                                     options=[{'label': i, 'value': i} for i in year_list],
                                                     placeholder="Select a year",
                                                     style={'width':'80%', 'padding':'3px', 'font-size': '20px', 'text-align-last' : 'center'}),
                                            # Place them next to each other using the division style
                                            ], style={'display': 'flex'}),  
                                          ]),
                                
                                # Add Computed graphs
                                # REVIEW3: Observe how we add an empty division and providing an id that will be updated during callback
                                html.Div([ ], id='plot1'),
    
                                html.Div([
                                        html.Div([ ], id='plot2'),
                                        html.Div([ ], id='plot3')
                                ], style={'display': 'flex'}),
                                
                                # TASK3: Add a division with two empty divisions inside. See above disvision for example.
                                # Enter your code below. Make sure you have correct formatting.
                               html.Div([
                                        html.Div([ ], id='plot4'),
                                        html.Div([ ], id='plot5')
                                ], style={'display': 'flex'}),

                                # Empty divisions for state condition when the user did not chose any input
                               html.Div([
                                        html.Div([ ], id='chart type'),
                                        html.Div([ ], id='year')
                                ], style={'display': 'flex'}),   
                                ])

# Callback function definition
# TASK4: Add 5 ouput components
# Enter your code below. Make sure you have correct formatting.
@app.callback([Output(component_id='plot1', component_property='children'),
                Output(component_id='plot2', component_property='children'),
                Output(component_id='plot3', component_property='children'),
                Output(component_id='plot4', component_property='children'),
                Output(component_id='plot5', component_property='children')],
               [Input(component_id='input-type', component_property='value'),
                Input(component_id='input-year', component_property='value')],
               # REVIEW4: Holding output state till user enters all the form information. In this case, it will be chart type and year
               [State("plot1", 'children'), State("plot2", "children"),
               State("plot3", "children"), State("plot4", "children"),
               State("plot5", "children")
               ])


# Add computation to callback function and return graph
def get_graph(chart, year, children1, children2, c3, c4, c5):
      
        # Select data
        df =  airline_data[airline_data['Year']==int(year)]
       
        if chart == 'OPT1':
            # Compute required information for creating graph from the data
            bar_data, line_data, div_data, map_data, tree_data = compute_data_choice_1(df)
            
            # Number of flights under different cancellation categories
            bar_fig = px.bar(bar_data, x='Month', y='Flights', color='CancellationCode', title='Monthly Flight Cancellation')
            
            # TASK5: Average flight time by reporting airline
            # Enter your code below. Make sure you have correct formatting.
            line_fig=px.line(line_data, x='Month', y='AirTime', color='Reporting_Airline', title='Average monthly flight time (minutes) by airline')
            
            # Percentage of diverted airport landings per reporting airline
            pie_fig = px.pie(div_data, values='Flights', names='Reporting_Airline', title='% of flights by reporting airline')
            
            # REVIEW5: Number of flights flying from each state using choropleth
            map_fig = px.choropleth(map_data,  # Input data
                    locations='OriginState', 
                    color='Flights',  
                    hover_data=['OriginState', 'Flights'], 
                    locationmode = 'USA-states', # Set to plot as US States
                    color_continuous_scale='GnBu',
                    range_color=[0, map_data['Flights'].max()]) 
            map_fig.update_layout(
                    title_text = 'Number of flights from origin state', 
                    geo_scope='usa') # Plot only the USA instead of globe
            
            # TASK6: Number of flights flying to each state from each reporting airline
            # Enter your code below. Make sure you have correct formatting.
            tree_fig = px.treemap(tree_data, path=['DestState', 'Reporting_Airline'], 
                      values='Flights',
                      color='Flights',
                      color_continuous_scale='RdBu',
                      title='Flight count by airline to destination state'
                )
            
            # REVIEW6: Return dcc.Graph component to the empty division
            return [dcc.Graph(figure=tree_fig), 
                    dcc.Graph(figure=pie_fig),
                    dcc.Graph(figure=map_fig),
                    dcc.Graph(figure=bar_fig),
                    dcc.Graph(figure=line_fig)
                   ]
        else:
            # REVIEW7: This covers chart type 2 and we have completed this exercise under Flight Delay Time Statistics Dashboard section
            # Compute required information for creating graph from the data
            avg_car, avg_weather, avg_NAS, avg_sec, avg_late = compute_data_choice_2(df)
            
            # Create graph
            carrier_fig = px.line(avg_car, x='Month', y='CarrierDelay', color='Reporting_Airline', title='Average carrrier delay time (minutes) by airline')
            weather_fig = px.line(avg_weather, x='Month', y='WeatherDelay', color='Reporting_Airline', title='Average weather delay time (minutes) by airline')
            nas_fig = px.line(avg_NAS, x='Month', y='NASDelay', color='Reporting_Airline', title='Average NAS delay time (minutes) by airline')
            sec_fig = px.line(avg_sec, x='Month', y='SecurityDelay', color='Reporting_Airline', title='Average security delay time (minutes) by airline')
            late_fig = px.line(avg_late, x='Month', y='LateAircraftDelay', color='Reporting_Airline', title='Average late aircraft delay time (minutes) by airline')
            
            return[dcc.Graph(figure=carrier_fig), 
                   dcc.Graph(figure=weather_fig), 
                   dcc.Graph(figure=nas_fig), 
                   dcc.Graph(figure=sec_fig), 
                   dcc.Graph(figure=late_fig)]


# Run the app
if __name__ == '__main__':
    app.run_server()



# Run the Application
# Firstly, install pandas and dash using the following command
python3 -m pip install pandas dash
# Run the python file using the command
python3 5_Peer_Graded_Assignment_Questions.py
# Observe the port number shown in the terminal.
# Click on the Launch Application option from the side menu bar. Provide the port number and click OK
# Congratulations, you have successfully completed your application!